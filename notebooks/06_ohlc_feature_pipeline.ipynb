{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davydsadovskyy/GitBackedShit/crypto-prediction/venv_crypto_prediction/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /Users/davydsadovskyy/GitBackedShit/crypto-prediction/notebooks\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "# from src.load_transform_data import get_new_ethereum_ohlc, get_new_ethereum_ohlc_2\n",
    "from src.hopsworks_connections import pull_data, upload_data, pull_model\n",
    "import pandas_ta as ta\n",
    "import numpy as np\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# Print the current working directory\n",
    "current_working_directory = os.getcwd()\n",
    "print(\"Current Working Directory:\", current_working_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Earlier, I used to scrape data from coinlore.com, but I realized they update it several hours late every day, so now I use coinGecko API instead. This was my code for that..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the last 30 days of Ethereum OHLC data from coinlore.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### This is the scraping function that is used in github actions - it wont work locally unless you have chromeDriver installed #####\n",
    "# But if you want ChromeDriver locally, you can do this (Mac):\n",
    "# 1. brew install chromedriver\n",
    "# 2. xattr -d com.apple.quarantine $(which chromedriver)\n",
    "# 3. which chromedriver\n",
    "# 4. the above line give the path to chromedriver. Now you can go into src/load_transfrom_data.py, and change the path in the get_new_ethereum_ohlc() function.  \n",
    "# new_eth_ohlc = get_new_ethereum_ohlc()\n",
    "\n",
    "##### Use this function when you running locally. This one doesn't require chromeDriver\n",
    "# new_eth_ohlc = get_new_ethereum_ohlc_2()\n",
    "\n",
    "# new_eth_ohlc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean the new raw ethereum data so hopsworks accepts it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_value(value):\n",
    "#     \"\"\"\n",
    "#     Converts a string value to a float. Removes $ signs, and converts\n",
    "#     billion (bn), million (m), and thousand (K) values to their numeric equivalents.\n",
    "#     \"\"\"\n",
    "#     value = value.replace('$', '')  # Remove $ sign to simplify processing\n",
    "#     if value[-1].lower() == 'm':\n",
    "#         return float(value[:-1]) * 1_000_000\n",
    "#     elif value[-1].lower() == 'b':\n",
    "#         return float(value[:-1]) * 1_000_000_000\n",
    "#     elif value[-1].lower() == 'k':\n",
    "#         return float(value[:-1]) * 1_000\n",
    "#     elif value[-2:].lower() == 'bn':  # Handle 'bn' for billions\n",
    "#         return float(value[:-2]) * 1_000_000_000\n",
    "#     else:\n",
    "#         return float(value)\n",
    "\n",
    "# new_eth_ohlc.columns = [col.lower() for col in new_eth_ohlc.columns]\n",
    "# new_eth_ohlc.rename(columns={'volume(eth)': 'volume_eth', 'market cap': 'market_cap'}, inplace=True)\n",
    "\n",
    "# for col in ['open', 'high', 'low', 'close', 'volume', 'market_cap']:\n",
    "#     new_eth_ohlc[col] = new_eth_ohlc[col].apply(convert_value)\n",
    "\n",
    "# new_eth_ohlc['date'] = pd.to_datetime(new_eth_ohlc['date'])\n",
    "\n",
    "# new_eth_ohlc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine this latest raw Etherum data with existing raw Ethereum data in hopsworks, and save it back into hopsworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_eth_ohlc = pull_data('raw_ethereum_ohlc', 1, 'raw_ethereum_ohlc_view', 1)\n",
    "# raw_eth_ohlc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hopsowrks returns fucked date column object, with weird time zone formats, we need to standardize first\n",
    "# raw_eth_ohlc['date'] = pd.to_datetime(raw_eth_ohlc['date']).dt.tz_localize(None)\n",
    "# new_eth_ohlc['date'] = pd.to_datetime(new_eth_ohlc['date']).dt.tz_localize(None)\n",
    "\n",
    "# combined_raw_eth_ohlc = pd.concat([new_eth_ohlc, raw_eth_ohlc], ignore_index=True)\n",
    "# combined_raw_eth_ohlc = combined_raw_eth_ohlc.drop_duplicates(subset='date', keep='last')\n",
    "# combined_raw_eth_ohlc = combined_raw_eth_ohlc.sort_values(by='date')\n",
    "# combined_raw_eth_ohlc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload_data(combined_raw_eth_ohlc, 'raw_ethereum_ohlc', 1, 'Raw ethereum OHLC (open, high, low, close) data from coinlore.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get recent data using CoinGecko API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2024, 5, 16, 20, 50, 41, 306450)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_recent_data(crypto):\n",
    "    # 20:00, or 8pm in EST is 12:00 in UTC, which is the cycle by which crypto OHLC are defined\n",
    "    # end_date = datetime.now().replace(hour=20, minute=0, second=0, microsecond=0)\n",
    "    # ^^^ ONLY WHEN RUNING LOCALLY, on github actions this will retrieve different data, so just use datetime.now() and schedule the run at 8pm EST = 0:00 UTC\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=1)\n",
    "\n",
    "    url = f\"https://api.coingecko.com/api/v3/coins/{crypto}/market_chart/range\"\n",
    "    params = {\n",
    "        'vs_currency': 'usd',\n",
    "        'from': int(start_date.timestamp()),\n",
    "        'to': int(end_date.timestamp())\n",
    "    }\n",
    "    \n",
    "    response = safe_request(url, params)\n",
    "    \n",
    "    if response and response.status_code == 200:\n",
    "        data = response.json()\n",
    "        df = pd.DataFrame(data['prices'], columns=['date', 'price'])\n",
    "        df['date'] = pd.to_datetime(df['date'], unit='ms')\n",
    "        df['market_cap'] = pd.DataFrame(data['market_caps'])[1].values\n",
    "        df['volume_24h'] = pd.DataFrame(data['total_volumes'])[1].values\n",
    "    else:\n",
    "        print(f\"Failed to fetch data. Status code: {response.status_code}\" if response else \"Failed to fetch data; no response.\")\n",
    "    \n",
    "    # Include only the earliest day\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    # Step 2: Extract the date part from the 'datetime' column\n",
    "    df['no_hour_date'] = df['date'].dt.date\n",
    "    earliest_date = df['no_hour_date'].min()\n",
    "    filtered_df = df[df['no_hour_date'] == earliest_date]\n",
    "    filtered_df = filtered_df.drop(columns='no_hour_date')\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "def safe_request(url, params, retries=20, backoff_factor=0.5):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            elif response.status_code == 429:\n",
    "                # We're being rate-limited; back off and retry\n",
    "                sleep_time = backoff_factor * (2 ** i)\n",
    "                print(f\"Rate limit hit. Waiting {sleep_time:.2f} seconds before retrying...\")\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                # Other errors, break the retry loop and return None\n",
    "                print(f\"Request failed with status code {response.status_code}.\")\n",
    "                return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request exception: {e}. Retrying...\")\n",
    "            time.sleep(backoff_factor * (2 ** i))\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>market_cap</th>\n",
       "      <th>volume_24h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-16 00:51:34.780</td>\n",
       "      <td>3026.241765</td>\n",
       "      <td>3.635023e+11</td>\n",
       "      <td>1.412628e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-05-16 00:56:13.743</td>\n",
       "      <td>3024.958580</td>\n",
       "      <td>3.635023e+11</td>\n",
       "      <td>1.378731e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-05-16 01:00:58.750</td>\n",
       "      <td>3023.029835</td>\n",
       "      <td>3.635023e+11</td>\n",
       "      <td>1.385196e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-05-16 01:05:24.885</td>\n",
       "      <td>3022.783584</td>\n",
       "      <td>3.631165e+11</td>\n",
       "      <td>1.430199e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-05-16 01:10:44.883</td>\n",
       "      <td>3022.749840</td>\n",
       "      <td>3.631165e+11</td>\n",
       "      <td>1.434601e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>2024-05-16 23:36:43.200</td>\n",
       "      <td>2937.213516</td>\n",
       "      <td>3.527897e+11</td>\n",
       "      <td>1.170811e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>2024-05-16 23:40:56.495</td>\n",
       "      <td>2941.414602</td>\n",
       "      <td>3.532894e+11</td>\n",
       "      <td>1.221331e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>2024-05-16 23:45:59.082</td>\n",
       "      <td>2942.708743</td>\n",
       "      <td>3.532894e+11</td>\n",
       "      <td>1.183504e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>2024-05-16 23:50:48.168</td>\n",
       "      <td>2942.872113</td>\n",
       "      <td>3.535010e+11</td>\n",
       "      <td>1.205532e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>2024-05-16 23:56:02.095</td>\n",
       "      <td>2943.585128</td>\n",
       "      <td>3.535010e+11</td>\n",
       "      <td>1.103960e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>278 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date        price    market_cap    volume_24h\n",
       "0   2024-05-16 00:51:34.780  3026.241765  3.635023e+11  1.412628e+10\n",
       "1   2024-05-16 00:56:13.743  3024.958580  3.635023e+11  1.378731e+10\n",
       "2   2024-05-16 01:00:58.750  3023.029835  3.635023e+11  1.385196e+10\n",
       "3   2024-05-16 01:05:24.885  3022.783584  3.631165e+11  1.430199e+10\n",
       "4   2024-05-16 01:10:44.883  3022.749840  3.631165e+11  1.434601e+10\n",
       "..                      ...          ...           ...           ...\n",
       "273 2024-05-16 23:36:43.200  2937.213516  3.527897e+11  1.170811e+10\n",
       "274 2024-05-16 23:40:56.495  2941.414602  3.532894e+11  1.221331e+10\n",
       "275 2024-05-16 23:45:59.082  2942.708743  3.532894e+11  1.183504e+10\n",
       "276 2024-05-16 23:50:48.168  2942.872113  3.535010e+11  1.205532e+10\n",
       "277 2024-05-16 23:56:02.095  2943.585128  3.535010e+11  1.103960e+10\n",
       "\n",
       "[278 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eth_raw = fetch_recent_data(\"ethereum\")\n",
    "eth_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform this data into ohlc (open, high, low, close) data for the past day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>volume_eth</th>\n",
       "      <th>market_cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-16</td>\n",
       "      <td>3026.241765</td>\n",
       "      <td>3026.768367</td>\n",
       "      <td>2926.11264</td>\n",
       "      <td>2943.585128</td>\n",
       "      <td>1.103960e+10</td>\n",
       "      <td>3.703648e+06</td>\n",
       "      <td>3.535010e+11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date         open         high         low        close        volume  \\\n",
       "0 2024-05-16  3026.241765  3026.768367  2926.11264  2943.585128  1.103960e+10   \n",
       "\n",
       "     volume_eth    market_cap  \n",
       "0  3.703648e+06  3.535010e+11  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eth_new = eth_raw.resample('D', on='date').agg({\n",
    "    'price': ['first', 'max', 'min', 'last', 'mean'],\n",
    "    'volume_24h': 'last',\n",
    "    'market_cap': 'last'\n",
    "}).reset_index()\n",
    "eth_new.columns = ['date', 'open', 'high', 'low', 'close', 'avg_price', 'volume', 'market_cap']\n",
    "eth_new['volume_eth'] = eth_new['volume'] / eth_new['avg_price']\n",
    "eth_new = eth_new[['date', 'open', 'high', 'low', 'close', 'volume', 'volume_eth', 'market_cap']]\n",
    "\n",
    "eth_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine this recent data with what we already have in hopsworks feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection closed.\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "UserWarning: The installed hopsworks client version 3.4.4 may not be compatible with the connected Hopsworks backend version 3.7.1. \n",
      "To ensure compatibility please install the latest bug fix release matching the minor version of your backend (3.7) by running 'pip install hopsworks==3.7.*'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/711829\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "Feature view already existed. Skip creation.\n",
      "Finished: Reading data from Hopsworks, using ArrowFlight (1.31s) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VersionWarning: Incremented version to `33`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>volume_eth</th>\n",
       "      <th>market_cap</th>\n",
       "      <th>tmw_1_0_percent_increase_binary</th>\n",
       "      <th>tmw_percent_increase_to_avg_high_low</th>\n",
       "      <th>...</th>\n",
       "      <th>last_100_day_40th_to_50th_pct_spread_count</th>\n",
       "      <th>last_100_day_50th_to_60th_pct_spread_count</th>\n",
       "      <th>last_100_day_60th_to_70th_pct_spread_count</th>\n",
       "      <th>last_100_day_70th_to_80th_pct_spread_count</th>\n",
       "      <th>last_100_day_90th_to_95th_pct_spread_count</th>\n",
       "      <th>last_100_day_40th_pct_spread_count</th>\n",
       "      <th>last_100_day_50th_pct_spread_count</th>\n",
       "      <th>last_100_day_60th_pct_spread_count</th>\n",
       "      <th>last_100_day_80th_pct_spread_count</th>\n",
       "      <th>last_100_day_90th_pct_spread_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-11-15</td>\n",
       "      <td>0.891200</td>\n",
       "      <td>0.921500</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.906400</td>\n",
       "      <td>4.118000e+05</td>\n",
       "      <td>4.583650e+05</td>\n",
       "      <td>6.720000e+07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.033594</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-11-16</td>\n",
       "      <td>0.906200</td>\n",
       "      <td>0.944700</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.929000</td>\n",
       "      <td>6.209000e+05</td>\n",
       "      <td>6.764420e+05</td>\n",
       "      <td>6.860000e+07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.097955</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-11-17</td>\n",
       "      <td>0.924900</td>\n",
       "      <td>1.030000</td>\n",
       "      <td>0.905800</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>1.100000e+06</td>\n",
       "      <td>1.183690e+06</td>\n",
       "      <td>7.220000e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.009901</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-11-18</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>0.940500</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>6.811000e+05</td>\n",
       "      <td>6.919940e+05</td>\n",
       "      <td>7.360000e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.007323</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-11-19</td>\n",
       "      <td>0.988700</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.955500</td>\n",
       "      <td>4.435000e+05</td>\n",
       "      <td>4.558660e+05</td>\n",
       "      <td>7.280000e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.015908</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3081</th>\n",
       "      <td>2024-05-11</td>\n",
       "      <td>2909.000000</td>\n",
       "      <td>2935.000000</td>\n",
       "      <td>2894.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>5.900000e+09</td>\n",
       "      <td>2.033385e+06</td>\n",
       "      <td>3.569000e+11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010306</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3082</th>\n",
       "      <td>2024-05-12</td>\n",
       "      <td>2912.000000</td>\n",
       "      <td>2951.000000</td>\n",
       "      <td>2906.000000</td>\n",
       "      <td>2931.000000</td>\n",
       "      <td>5.200000e+09</td>\n",
       "      <td>1.779026e+06</td>\n",
       "      <td>3.581000e+11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010926</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083</th>\n",
       "      <td>2024-05-13</td>\n",
       "      <td>2916.796334</td>\n",
       "      <td>2984.551884</td>\n",
       "      <td>2868.448720</td>\n",
       "      <td>2948.303455</td>\n",
       "      <td>1.393028e+10</td>\n",
       "      <td>4.735937e+06</td>\n",
       "      <td>3.541556e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.009737</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3084</th>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>2954.422784</td>\n",
       "      <td>2957.396066</td>\n",
       "      <td>2867.874351</td>\n",
       "      <td>2881.795992</td>\n",
       "      <td>1.125873e+10</td>\n",
       "      <td>3.871148e+06</td>\n",
       "      <td>3.463060e+11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.054264</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3085</th>\n",
       "      <td>2024-05-15</td>\n",
       "      <td>2890.012735</td>\n",
       "      <td>3040.591286</td>\n",
       "      <td>2885.457706</td>\n",
       "      <td>3035.758955</td>\n",
       "      <td>1.380971e+10</td>\n",
       "      <td>4.681133e+06</td>\n",
       "      <td>3.652259e+11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3086 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date         open         high          low        close  \\\n",
       "0     2015-11-15     0.891200     0.921500     0.875000     0.906400   \n",
       "1     2015-11-16     0.906200     0.944700     0.892000     0.929000   \n",
       "2     2015-11-17     0.924900     1.030000     0.905800     1.010000   \n",
       "3     2015-11-18     0.990000     1.010000     0.940500     0.990000   \n",
       "4     2015-11-19     0.988700     1.010000     0.937500     0.955500   \n",
       "...          ...          ...          ...          ...          ...   \n",
       "3081  2024-05-11  2909.000000  2935.000000  2894.000000  2911.000000   \n",
       "3082  2024-05-12  2912.000000  2951.000000  2906.000000  2931.000000   \n",
       "3083  2024-05-13  2916.796334  2984.551884  2868.448720  2948.303455   \n",
       "3084  2024-05-14  2954.422784  2957.396066  2867.874351  2881.795992   \n",
       "3085  2024-05-15  2890.012735  3040.591286  2885.457706  3035.758955   \n",
       "\n",
       "            volume    volume_eth    market_cap  \\\n",
       "0     4.118000e+05  4.583650e+05  6.720000e+07   \n",
       "1     6.209000e+05  6.764420e+05  6.860000e+07   \n",
       "2     1.100000e+06  1.183690e+06  7.220000e+07   \n",
       "3     6.811000e+05  6.919940e+05  7.360000e+07   \n",
       "4     4.435000e+05  4.558660e+05  7.280000e+07   \n",
       "...            ...           ...           ...   \n",
       "3081  5.900000e+09  2.033385e+06  3.569000e+11   \n",
       "3082  5.200000e+09  1.779026e+06  3.581000e+11   \n",
       "3083  1.393028e+10  4.735937e+06  3.541556e+11   \n",
       "3084  1.125873e+10  3.871148e+06  3.463060e+11   \n",
       "3085  1.380971e+10  4.681133e+06  3.652259e+11   \n",
       "\n",
       "      tmw_1_0_percent_increase_binary  tmw_percent_increase_to_avg_high_low  \\\n",
       "0                                 1.0                              0.033594   \n",
       "1                                 1.0                              0.097955   \n",
       "2                                 0.0                             -0.009901   \n",
       "3                                 0.0                             -0.007323   \n",
       "4                                 0.0                             -0.015908   \n",
       "...                               ...                                   ...   \n",
       "3081                              1.0                              0.010306   \n",
       "3082                              1.0                              0.010926   \n",
       "3083                              0.0                             -0.009737   \n",
       "3084                              1.0                              0.054264   \n",
       "3085                              NaN                                   NaN   \n",
       "\n",
       "      ...  last_100_day_40th_to_50th_pct_spread_count  \\\n",
       "0     ...                                         8.0   \n",
       "1     ...                                         8.0   \n",
       "2     ...                                         8.0   \n",
       "3     ...                                         8.0   \n",
       "4     ...                                         8.0   \n",
       "...   ...                                         ...   \n",
       "3081  ...                                        14.0   \n",
       "3082  ...                                        14.0   \n",
       "3083  ...                                        14.0   \n",
       "3084  ...                                        14.0   \n",
       "3085  ...                                        14.0   \n",
       "\n",
       "      last_100_day_50th_to_60th_pct_spread_count  \\\n",
       "0                                            7.0   \n",
       "1                                            8.0   \n",
       "2                                            8.0   \n",
       "3                                            8.0   \n",
       "4                                            8.0   \n",
       "...                                          ...   \n",
       "3081                                         9.0   \n",
       "3082                                         9.0   \n",
       "3083                                         9.0   \n",
       "3084                                         9.0   \n",
       "3085                                        10.0   \n",
       "\n",
       "      last_100_day_60th_to_70th_pct_spread_count  \\\n",
       "0                                            6.0   \n",
       "1                                            6.0   \n",
       "2                                            6.0   \n",
       "3                                            7.0   \n",
       "4                                            8.0   \n",
       "...                                          ...   \n",
       "3081                                        10.0   \n",
       "3082                                        10.0   \n",
       "3083                                        10.0   \n",
       "3084                                        10.0   \n",
       "3085                                        10.0   \n",
       "\n",
       "      last_100_day_70th_to_80th_pct_spread_count  \\\n",
       "0                                           18.0   \n",
       "1                                           18.0   \n",
       "2                                           18.0   \n",
       "3                                           18.0   \n",
       "4                                           18.0   \n",
       "...                                          ...   \n",
       "3081                                         8.0   \n",
       "3082                                         8.0   \n",
       "3083                                         8.0   \n",
       "3084                                         8.0   \n",
       "3085                                         8.0   \n",
       "\n",
       "      last_100_day_90th_to_95th_pct_spread_count  \\\n",
       "0                                           18.0   \n",
       "1                                           18.0   \n",
       "2                                           18.0   \n",
       "3                                           17.0   \n",
       "4                                           17.0   \n",
       "...                                          ...   \n",
       "3081                                         3.0   \n",
       "3082                                         3.0   \n",
       "3083                                         3.0   \n",
       "3084                                         3.0   \n",
       "3085                                         3.0   \n",
       "\n",
       "      last_100_day_40th_pct_spread_count  last_100_day_50th_pct_spread_count  \\\n",
       "0                                   94.0                                86.0   \n",
       "1                                   94.0                                86.0   \n",
       "2                                   94.0                                86.0   \n",
       "3                                   94.0                                86.0   \n",
       "4                                   94.0                                86.0   \n",
       "...                                  ...                                 ...   \n",
       "3081                                48.0                                34.0   \n",
       "3082                                48.0                                34.0   \n",
       "3083                                48.0                                34.0   \n",
       "3084                                48.0                                34.0   \n",
       "3085                                49.0                                35.0   \n",
       "\n",
       "      last_100_day_60th_pct_spread_count  last_100_day_80th_pct_spread_count  \\\n",
       "0                                   79.0                                55.0   \n",
       "1                                   78.0                                54.0   \n",
       "2                                   78.0                                54.0   \n",
       "3                                   78.0                                53.0   \n",
       "4                                   78.0                                52.0   \n",
       "...                                  ...                                 ...   \n",
       "3081                                25.0                                 7.0   \n",
       "3082                                25.0                                 7.0   \n",
       "3083                                25.0                                 7.0   \n",
       "3084                                25.0                                 7.0   \n",
       "3085                                25.0                                 7.0   \n",
       "\n",
       "      last_100_day_90th_pct_spread_count  \n",
       "0                                   46.0  \n",
       "1                                   45.0  \n",
       "2                                   44.0  \n",
       "3                                   43.0  \n",
       "4                                   42.0  \n",
       "...                                  ...  \n",
       "3081                                 3.0  \n",
       "3082                                 3.0  \n",
       "3083                                 3.0  \n",
       "3084                                 3.0  \n",
       "3085                                 3.0  \n",
       "\n",
       "[3086 rows x 106 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eth_old = pull_data('eth_ohlc_transformed', 2, 'eth_ohlc_transformed_view', 2)\n",
    "eth_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eth_old = eth_old[:-1]\n",
    "# eth_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the raw, untransformed portion of the old date with the new data we just got from CoinGecko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>volume_eth</th>\n",
       "      <th>market_cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-11-15</td>\n",
       "      <td>0.891200</td>\n",
       "      <td>0.921500</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.906400</td>\n",
       "      <td>4.118000e+05</td>\n",
       "      <td>4.583650e+05</td>\n",
       "      <td>6.720000e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-11-16</td>\n",
       "      <td>0.906200</td>\n",
       "      <td>0.944700</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.929000</td>\n",
       "      <td>6.209000e+05</td>\n",
       "      <td>6.764420e+05</td>\n",
       "      <td>6.860000e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-11-17</td>\n",
       "      <td>0.924900</td>\n",
       "      <td>1.030000</td>\n",
       "      <td>0.905800</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>1.100000e+06</td>\n",
       "      <td>1.183690e+06</td>\n",
       "      <td>7.220000e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-11-18</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>0.940500</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>6.811000e+05</td>\n",
       "      <td>6.919940e+05</td>\n",
       "      <td>7.360000e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-11-19</td>\n",
       "      <td>0.988700</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.955500</td>\n",
       "      <td>4.435000e+05</td>\n",
       "      <td>4.558660e+05</td>\n",
       "      <td>7.280000e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3082</th>\n",
       "      <td>2024-05-12</td>\n",
       "      <td>2912.000000</td>\n",
       "      <td>2951.000000</td>\n",
       "      <td>2906.000000</td>\n",
       "      <td>2931.000000</td>\n",
       "      <td>5.200000e+09</td>\n",
       "      <td>1.779026e+06</td>\n",
       "      <td>3.581000e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083</th>\n",
       "      <td>2024-05-13</td>\n",
       "      <td>2916.796334</td>\n",
       "      <td>2984.551884</td>\n",
       "      <td>2868.448720</td>\n",
       "      <td>2948.303455</td>\n",
       "      <td>1.393028e+10</td>\n",
       "      <td>4.735937e+06</td>\n",
       "      <td>3.541556e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3084</th>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>2954.422784</td>\n",
       "      <td>2957.396066</td>\n",
       "      <td>2867.874351</td>\n",
       "      <td>2881.795992</td>\n",
       "      <td>1.125873e+10</td>\n",
       "      <td>3.871148e+06</td>\n",
       "      <td>3.463060e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3085</th>\n",
       "      <td>2024-05-15</td>\n",
       "      <td>2890.012735</td>\n",
       "      <td>3040.591286</td>\n",
       "      <td>2885.457706</td>\n",
       "      <td>3035.758955</td>\n",
       "      <td>1.380971e+10</td>\n",
       "      <td>4.681133e+06</td>\n",
       "      <td>3.652259e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3086</th>\n",
       "      <td>2024-05-16</td>\n",
       "      <td>3026.241765</td>\n",
       "      <td>3026.768367</td>\n",
       "      <td>2926.112640</td>\n",
       "      <td>2943.585128</td>\n",
       "      <td>1.103960e+10</td>\n",
       "      <td>3.703648e+06</td>\n",
       "      <td>3.535010e+11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3087 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date         open         high          low        close  \\\n",
       "0    2015-11-15     0.891200     0.921500     0.875000     0.906400   \n",
       "1    2015-11-16     0.906200     0.944700     0.892000     0.929000   \n",
       "2    2015-11-17     0.924900     1.030000     0.905800     1.010000   \n",
       "3    2015-11-18     0.990000     1.010000     0.940500     0.990000   \n",
       "4    2015-11-19     0.988700     1.010000     0.937500     0.955500   \n",
       "...         ...          ...          ...          ...          ...   \n",
       "3082 2024-05-12  2912.000000  2951.000000  2906.000000  2931.000000   \n",
       "3083 2024-05-13  2916.796334  2984.551884  2868.448720  2948.303455   \n",
       "3084 2024-05-14  2954.422784  2957.396066  2867.874351  2881.795992   \n",
       "3085 2024-05-15  2890.012735  3040.591286  2885.457706  3035.758955   \n",
       "3086 2024-05-16  3026.241765  3026.768367  2926.112640  2943.585128   \n",
       "\n",
       "            volume    volume_eth    market_cap  \n",
       "0     4.118000e+05  4.583650e+05  6.720000e+07  \n",
       "1     6.209000e+05  6.764420e+05  6.860000e+07  \n",
       "2     1.100000e+06  1.183690e+06  7.220000e+07  \n",
       "3     6.811000e+05  6.919940e+05  7.360000e+07  \n",
       "4     4.435000e+05  4.558660e+05  7.280000e+07  \n",
       "...            ...           ...           ...  \n",
       "3082  5.200000e+09  1.779026e+06  3.581000e+11  \n",
       "3083  1.393028e+10  4.735937e+06  3.541556e+11  \n",
       "3084  1.125873e+10  3.871148e+06  3.463060e+11  \n",
       "3085  1.380971e+10  4.681133e+06  3.652259e+11  \n",
       "3086  1.103960e+10  3.703648e+06  3.535010e+11  \n",
       "\n",
       "[3087 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_needed = ['date', 'open', 'high', 'low', 'close', 'volume', 'volume_eth', 'market_cap']\n",
    "\n",
    "# Convert date columns to datetime objects, in case they are not already\n",
    "eth_old['date'] = pd.to_datetime(eth_old['date'])\n",
    "eth_new['date'] = pd.to_datetime(eth_new['date'])\n",
    "\n",
    "# Remove any timezone information to make the comparison straightforward\n",
    "eth_old['date'] = eth_old['date'].dt.tz_localize(None)\n",
    "eth_new['date'] = eth_new['date'].dt.tz_localize(None)\n",
    "\n",
    "if not any(eth_new['date'].isin(eth_old['date'])):\n",
    "    # Since eth_new's date isn't found, just concatenate it\n",
    "    eth_combined = pd.concat([eth_old[columns_needed], eth_new], ignore_index=True)\n",
    "else:\n",
    "    # If the date is found, remove the last row from eth_old and add on the new row\n",
    "    eth_combined = pd.concat([eth_old.iloc[:-1], eth_new], ignore_index=True)\n",
    "\n",
    "eth_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this combined raw dataset to regenerate all the response and predictor variables for the past observations, and for the new observation that was just added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_response_vars(data):\n",
    "    df = data.copy()\n",
    "    tmw_avg_high_close = (df[\"high\"].shift(-1) + df[\"close\"].shift(-1)) / 2\n",
    "    df['tmw_percent_increase_to_avg_high_low'] = ((tmw_avg_high_close - df['close']) / df['close'])\n",
    "    df['tmw_1_0_percent_increase_binary'] = (df['tmw_percent_increase_to_avg_high_low'] >= .01).astype(int)\n",
    "\n",
    "    # Last row doesn't have a value for tomorrow's return, but it was assigned 0's for response variable. Convert them to NA\n",
    "    df.loc[df.index[-1], ['tmw_percent_increase_to_avg_high_low', 'tmw_1_0_percent_increase_binary']] = pd.NA\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_predictor_vars(data):\n",
    "    df = data.copy()\n",
    "\n",
    "    # define 'helper' columns\n",
    "    perc_from_low_to_high = (df['high'] - df['low']) / df['low']\n",
    "\n",
    "    tmw_vol = df[\"volume\"].shift(-1)\n",
    "    tmw_perc_change_vol = (tmw_vol - df['volume']) / df['volume']\n",
    "\n",
    "\n",
    "    # Create binary columns for each month (1-12)\n",
    "    month_names = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december']\n",
    "    for i in range(1, 13):  # Months are typically 1-12\n",
    "        month_name = month_names[i-1]  # Get the month name from the list\n",
    "        df[f'{month_name}'] = (df['date'].dt.month == i).astype(int)\n",
    "\n",
    "    # Create binary columns for each day\n",
    "    days = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "    for i, day in enumerate(days):\n",
    "        df[f'{day.lower()}'] = (df['date'].dt.dayofweek == i).astype(int)\n",
    "\n",
    "    horizons = [2,5,10,25,50,100] \n",
    "    for horizon in horizons:\n",
    "\n",
    "        # Ratio - Close Price Simple Moving Average (SMA)\n",
    "        sma_col = f\"sma_{horizon}\"\n",
    "        df[sma_col] = df[\"close\"] / ta.sma(df[\"close\"], length=horizon)\n",
    "        # Close Price Relative Strength Index (RSI)\n",
    "        rsi_col = f\"rsi_{horizon}\"\n",
    "        df[rsi_col] = ta.rsi(df[\"close\"], length=horizon)\n",
    "\n",
    "        # Ratio - Volume Simple Moving Average (SMA)\n",
    "        sma_col = f\"volume_sma_{horizon}\"\n",
    "        df[sma_col] = df[\"volume\"] / ta.sma(df[\"volume\"], length=horizon)\n",
    "        # Volume Relative Strength Index (RSI)\n",
    "        rsi_col = f\"volume_rsi_{horizon}\"\n",
    "        df[rsi_col] = ta.rsi(df[\"volume\"], length=horizon)\n",
    "\n",
    "        # Ratio - High-Low Spread Simple Moving Average (SMA)\n",
    "        sma_col = f\"spread_sma_{horizon}\"\n",
    "        df[sma_col] = perc_from_low_to_high / ta.sma(perc_from_low_to_high, length=horizon)\n",
    "        # High-Low Spread Relative Strength Index (RSI)\n",
    "        rsi_col = f\"spread_rsi_{horizon}\"\n",
    "        df[rsi_col] = ta.rsi(perc_from_low_to_high, length=horizon)\n",
    "\n",
    "        # Sum of the number of days in the past horizon that had a certain percent change in price from a day's close to the next day's (high+close)/2\n",
    "        df[f\"last_{horizon}_day_40th_to_50th_pct_price_change_count\"] = ((df['tmw_percent_increase_to_avg_high_low'] > 0.002684) & (df['tmw_percent_increase_to_avg_high_low'] <= 0.008635)).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_50th_to_60th_pct_price_change_count\"] = ((df['tmw_percent_increase_to_avg_high_low'] > 0.008635) & (df['tmw_percent_increase_to_avg_high_low'] <= 0.016393)).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_60th_to_70th_pct_price_change_count\"] = ((df['tmw_percent_increase_to_avg_high_low'] > 0.016393) & (df['tmw_percent_increase_to_avg_high_low'] <= 0.026398)).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_70th_to_80th_pct_price_change_count\"] = ((df['tmw_percent_increase_to_avg_high_low'] > 0.026398) & (df['tmw_percent_increase_to_avg_high_low'] <= 0.041709)).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_80th_to_90th_pct_price_change_count\"] = ((df['tmw_percent_increase_to_avg_high_low'] > 0.041709) & (df['tmw_percent_increase_to_avg_high_low'] <= 0.070177)).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_90th_to_95th_pct_price_change_count\"] = ((df['tmw_percent_increase_to_avg_high_low'] > 0.070177) & (df['tmw_percent_increase_to_avg_high_low'] <= 0.106177)).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_40th_pct_price_change_count\"] = (df['tmw_percent_increase_to_avg_high_low'] > 0.002684).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_50th_pct_price_change_count\"] = (df['tmw_percent_increase_to_avg_high_low'] > 0.008635).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_60th_pct_price_change_count\"] = (df['tmw_percent_increase_to_avg_high_low'] > 0.016393).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_70th_pct_price_change_count\"] = (df['tmw_percent_increase_to_avg_high_low'] > 0.026398).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_80th_pct_price_change_count\"] = (df['tmw_percent_increase_to_avg_high_low'] > 0.041709).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_90th_pct_price_change_count\"] = (df['tmw_percent_increase_to_avg_high_low'] > 0.070177).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_95th_pct_price_change_count\"] = (df['tmw_percent_increase_to_avg_high_low'] > 0.106177).astype(int).shift(1).rolling(horizon).sum()\n",
    "                \n",
    "        \n",
    "        # Sum of the number of days in the past horizon that had a certain percent chamge in volume from one day to the next\n",
    "        tmw_vol = df[\"volume\"].shift(-1)\n",
    "        tmw_perc_change_vol = (tmw_vol - df['volume']) / df['volume']\n",
    "        df[f\"last_{horizon}_day_40th_to_50th_pct_volume_change_count\"] = ((tmw_perc_change_vol > -0.069444) & (tmw_perc_change_vol <= -0.008197)).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_50th_to_60th_pct_volume_change_count\"] = ((tmw_perc_change_vol > -0.008197) & (tmw_perc_change_vol <= 0.061069)).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_60th_to_70th_pct_volume_change_count\"] = ((tmw_perc_change_vol > 0.061069) & (tmw_perc_change_vol <= 0.137500)).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_70th_to_80th_pct_volume_change_count\"] = ((tmw_perc_change_vol > 0.137500) & (tmw_perc_change_vol <= 0.276316)).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_80th_to_90th_pct_volume_change_count\"] = ((tmw_perc_change_vol > 0.276316) & (tmw_perc_change_vol <= 0.544118)).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_90th_to_95th_pct_volume_change_count\"] = ((tmw_perc_change_vol > 0.544118) & (tmw_perc_change_vol <= 0.885965)).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_40th_pct_volume_change_count\"] = (tmw_perc_change_vol > -0.069444).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_50th_pct_volume_change_count\"] = (tmw_perc_change_vol > -0.008197).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_60th_pct_volume_change_count\"] = (tmw_perc_change_vol > 0.061069).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_70th_pct_volume_change_count\"] = (tmw_perc_change_vol > 0.137500).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_80th_pct_volume_change_count\"] = (tmw_perc_change_vol > 0.276316).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_90th_pct_volume_change_count\"] = (tmw_perc_change_vol > 0.544118).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_95th_pct_volume_change_count\"] = (tmw_perc_change_vol > 0.885965).astype(int).shift(1).rolling(horizon).sum()\n",
    "\n",
    "        # Sum of the number of days in the past horizon that had a certain percent chamge in the difference between low and high price\n",
    "        prc_from_low_to_high = (df['high'] - df['low']) / df['low']\n",
    "        df[f\"last_{horizon}_day_40th_to_50th_pct_spread_count\"] = ((prc_from_low_to_high > 0.043674) & (prc_from_low_to_high <= 0.053125)).astype(int).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_50th_to_60th_pct_spread_count\"] = ((prc_from_low_to_high > 0.053125) & (prc_from_low_to_high <= 0.064163)).astype(int).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_60th_to_70th_pct_spread_count\"] = ((prc_from_low_to_high > 0.064163) & (prc_from_low_to_high <= 0.077524)).astype(int).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_70th_to_80th_pct_spread_count\"] = ((prc_from_low_to_high > 0.077524) & (prc_from_low_to_high <= 0.097901)).astype(int).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_80th_to_90th_pct_spread_count\"] = ((prc_from_low_to_high > 0.097901) & (prc_from_low_to_high <= 0.137338)).astype(int).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_90th_to_95th_pct_spread_count\"] = ((prc_from_low_to_high > 0.137338) & (prc_from_low_to_high <= 0.191936)).astype(int).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_40th_pct_spread_count\"] = (prc_from_low_to_high > 0.043674).astype(int).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_50th_pct_spread_count\"] = (prc_from_low_to_high > 0.053125).astype(int).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_60th_pct_spread_count\"] = (prc_from_low_to_high > 0.064163).astype(int).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_70th_pct_spread_count\"] = (prc_from_low_to_high > 0.077524).astype(int).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_80th_pct_spread_count\"] = (prc_from_low_to_high > 0.097901).astype(int).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_90th_pct_spread_count\"] = (prc_from_low_to_high > 0.137338).astype(int).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_95th_pct_spread_count\"] = (prc_from_low_to_high > 0.191936).astype(int).rolling(horizon).sum()\n",
    "\n",
    "    # only keep the 96 predictor set, the one we found to be the best\n",
    "    predictors_96 = ['rsi_2', 'volume_rsi_2', 'spread_rsi_2', 'last_2_day_80th_pct_volume_change_count', 'last_2_day_60th_to_70th_pct_spread_count', 'last_2_day_40th_pct_spread_count', 'rsi_5', 'volume_rsi_5', 'spread_rsi_5', 'last_5_day_60th_pct_price_change_count', 'last_5_day_70th_pct_price_change_count', 'rsi_10', 'volume_rsi_10', 'spread_rsi_10', 'last_10_day_40th_to_50th_pct_price_change_count', 'last_10_day_60th_pct_price_change_count', 'last_10_day_70th_to_80th_pct_volume_change_count', 'last_10_day_80th_to_90th_pct_spread_count', 'last_10_day_40th_pct_spread_count', 'last_10_day_50th_pct_spread_count', 'last_10_day_70th_pct_spread_count', 'last_10_day_80th_pct_spread_count', 'rsi_25', 'volume_rsi_25', 'spread_rsi_25', 'last_25_day_40th_to_50th_pct_price_change_count', 'last_25_day_70th_to_80th_pct_price_change_count', 'last_25_day_80th_to_90th_pct_price_change_count', 'last_25_day_40th_pct_price_change_count', 'last_25_day_50th_pct_price_change_count', 'last_25_day_80th_pct_price_change_count', 'last_25_day_40th_to_50th_pct_volume_change_count', 'last_25_day_50th_to_60th_pct_volume_change_count', 'last_25_day_60th_to_70th_pct_volume_change_count', 'last_25_day_70th_to_80th_pct_volume_change_count', 'last_25_day_80th_to_90th_pct_volume_change_count', 'last_25_day_50th_pct_volume_change_count', 'last_25_day_60th_pct_volume_change_count', 'last_25_day_70th_pct_volume_change_count', 'last_25_day_80th_pct_volume_change_count', 'last_25_day_40th_to_50th_pct_spread_count', 'last_25_day_50th_to_60th_pct_spread_count', 'last_25_day_60th_to_70th_pct_spread_count', 'last_25_day_70th_to_80th_pct_spread_count', 'last_25_day_80th_to_90th_pct_spread_count', 'last_25_day_40th_pct_spread_count', 'last_25_day_60th_pct_spread_count', 'last_25_day_70th_pct_spread_count', 'last_25_day_90th_pct_spread_count', 'rsi_50', 'last_50_day_50th_to_60th_pct_price_change_count', 'last_50_day_60th_to_70th_pct_price_change_count', 'last_50_day_70th_to_80th_pct_price_change_count', 'last_50_day_40th_pct_price_change_count', 'last_50_day_50th_pct_price_change_count', 'last_50_day_70th_pct_price_change_count', 'last_50_day_40th_to_50th_pct_volume_change_count', 'last_50_day_50th_to_60th_pct_volume_change_count', 'last_50_day_50th_pct_volume_change_count', 'last_50_day_80th_pct_volume_change_count', 'last_50_day_40th_to_50th_pct_spread_count', 'last_50_day_50th_to_60th_pct_spread_count', 'last_50_day_60th_to_70th_pct_spread_count', 'last_50_day_70th_to_80th_pct_spread_count', 'last_50_day_80th_to_90th_pct_spread_count', 'last_50_day_40th_pct_spread_count', 'last_50_day_50th_pct_spread_count', 'last_50_day_70th_pct_spread_count', 'last_50_day_90th_pct_spread_count', 'rsi_100', 'last_100_day_60th_to_70th_pct_price_change_count', 'last_100_day_80th_to_90th_pct_price_change_count', 'last_100_day_90th_to_95th_pct_price_change_count', 'last_100_day_40th_pct_price_change_count', 'last_100_day_50th_pct_price_change_count', 'last_100_day_60th_pct_price_change_count', 'last_100_day_80th_pct_price_change_count', 'last_100_day_40th_to_50th_pct_volume_change_count', 'last_100_day_50th_to_60th_pct_volume_change_count', 'last_100_day_60th_to_70th_pct_volume_change_count', 'last_100_day_70th_to_80th_pct_volume_change_count', 'last_100_day_80th_to_90th_pct_volume_change_count', 'last_100_day_40th_pct_volume_change_count', 'last_100_day_60th_pct_volume_change_count', 'last_100_day_80th_pct_volume_change_count', 'last_100_day_90th_pct_volume_change_count', 'last_100_day_40th_to_50th_pct_spread_count', 'last_100_day_50th_to_60th_pct_spread_count', 'last_100_day_60th_to_70th_pct_spread_count', 'last_100_day_70th_to_80th_pct_spread_count', 'last_100_day_90th_to_95th_pct_spread_count', 'last_100_day_40th_pct_spread_count', 'last_100_day_50th_pct_spread_count', 'last_100_day_60th_pct_spread_count', 'last_100_day_80th_pct_spread_count', 'last_100_day_90th_pct_spread_count']\n",
    "    cols_for_hopsworks = ['date', 'open', 'high', 'low', 'close', 'volume', 'volume_eth', 'market_cap']\n",
    "    df = df[cols_for_hopsworks + ['tmw_1_0_percent_increase_binary', 'tmw_percent_increase_to_avg_high_low'] + predictors_96]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>volume_eth</th>\n",
       "      <th>market_cap</th>\n",
       "      <th>tmw_1_0_percent_increase_binary</th>\n",
       "      <th>tmw_percent_increase_to_avg_high_low</th>\n",
       "      <th>...</th>\n",
       "      <th>last_100_day_40th_to_50th_pct_spread_count</th>\n",
       "      <th>last_100_day_50th_to_60th_pct_spread_count</th>\n",
       "      <th>last_100_day_60th_to_70th_pct_spread_count</th>\n",
       "      <th>last_100_day_70th_to_80th_pct_spread_count</th>\n",
       "      <th>last_100_day_90th_to_95th_pct_spread_count</th>\n",
       "      <th>last_100_day_40th_pct_spread_count</th>\n",
       "      <th>last_100_day_50th_pct_spread_count</th>\n",
       "      <th>last_100_day_60th_pct_spread_count</th>\n",
       "      <th>last_100_day_80th_pct_spread_count</th>\n",
       "      <th>last_100_day_90th_pct_spread_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3085</th>\n",
       "      <td>2024-05-15</td>\n",
       "      <td>2890.012735</td>\n",
       "      <td>3040.591286</td>\n",
       "      <td>2885.457706</td>\n",
       "      <td>3035.758955</td>\n",
       "      <td>1.380971e+10</td>\n",
       "      <td>4.681133e+06</td>\n",
       "      <td>3.652259e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.016662</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3086</th>\n",
       "      <td>2024-05-16</td>\n",
       "      <td>3026.241765</td>\n",
       "      <td>3026.768367</td>\n",
       "      <td>2926.112640</td>\n",
       "      <td>2943.585128</td>\n",
       "      <td>1.103960e+10</td>\n",
       "      <td>3.703648e+06</td>\n",
       "      <td>3.535010e+11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date         open         high          low        close  \\\n",
       "3085 2024-05-15  2890.012735  3040.591286  2885.457706  3035.758955   \n",
       "3086 2024-05-16  3026.241765  3026.768367  2926.112640  2943.585128   \n",
       "\n",
       "            volume    volume_eth    market_cap  \\\n",
       "3085  1.380971e+10  4.681133e+06  3.652259e+11   \n",
       "3086  1.103960e+10  3.703648e+06  3.535010e+11   \n",
       "\n",
       "      tmw_1_0_percent_increase_binary  tmw_percent_increase_to_avg_high_low  \\\n",
       "3085                              0.0                             -0.016662   \n",
       "3086                              NaN                                   NaN   \n",
       "\n",
       "      ...  last_100_day_40th_to_50th_pct_spread_count  \\\n",
       "3085  ...                                        14.0   \n",
       "3086  ...                                        14.0   \n",
       "\n",
       "      last_100_day_50th_to_60th_pct_spread_count  \\\n",
       "3085                                        10.0   \n",
       "3086                                        10.0   \n",
       "\n",
       "      last_100_day_60th_to_70th_pct_spread_count  \\\n",
       "3085                                        10.0   \n",
       "3086                                        10.0   \n",
       "\n",
       "      last_100_day_70th_to_80th_pct_spread_count  \\\n",
       "3085                                         8.0   \n",
       "3086                                         8.0   \n",
       "\n",
       "      last_100_day_90th_to_95th_pct_spread_count  \\\n",
       "3085                                         3.0   \n",
       "3086                                         3.0   \n",
       "\n",
       "      last_100_day_40th_pct_spread_count  last_100_day_50th_pct_spread_count  \\\n",
       "3085                                49.0                                35.0   \n",
       "3086                                49.0                                35.0   \n",
       "\n",
       "      last_100_day_60th_pct_spread_count  last_100_day_80th_pct_spread_count  \\\n",
       "3085                                25.0                                 7.0   \n",
       "3086                                25.0                                 7.0   \n",
       "\n",
       "      last_100_day_90th_pct_spread_count  \n",
       "3085                                 3.0  \n",
       "3086                                 3.0  \n",
       "\n",
       "[2 rows x 106 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eth_responses = add_response_vars(eth_combined)\n",
    "eth_predictors_and_responses = add_predictor_vars(eth_responses)\n",
    "eth_new_days_transformed = eth_predictors_and_responses.iloc[-2:]\n",
    "eth_new_days_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>volume_eth</th>\n",
       "      <th>market_cap</th>\n",
       "      <th>tmw_1_0_percent_increase_binary</th>\n",
       "      <th>tmw_percent_increase_to_avg_high_low</th>\n",
       "      <th>...</th>\n",
       "      <th>last_100_day_40th_to_50th_pct_spread_count</th>\n",
       "      <th>last_100_day_50th_to_60th_pct_spread_count</th>\n",
       "      <th>last_100_day_60th_to_70th_pct_spread_count</th>\n",
       "      <th>last_100_day_70th_to_80th_pct_spread_count</th>\n",
       "      <th>last_100_day_90th_to_95th_pct_spread_count</th>\n",
       "      <th>last_100_day_40th_pct_spread_count</th>\n",
       "      <th>last_100_day_50th_pct_spread_count</th>\n",
       "      <th>last_100_day_60th_pct_spread_count</th>\n",
       "      <th>last_100_day_80th_pct_spread_count</th>\n",
       "      <th>last_100_day_90th_pct_spread_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-11-15</td>\n",
       "      <td>0.891200</td>\n",
       "      <td>0.921500</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.906400</td>\n",
       "      <td>4.118000e+05</td>\n",
       "      <td>4.583650e+05</td>\n",
       "      <td>6.720000e+07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.033594</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-11-16</td>\n",
       "      <td>0.906200</td>\n",
       "      <td>0.944700</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.929000</td>\n",
       "      <td>6.209000e+05</td>\n",
       "      <td>6.764420e+05</td>\n",
       "      <td>6.860000e+07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.097955</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-11-17</td>\n",
       "      <td>0.924900</td>\n",
       "      <td>1.030000</td>\n",
       "      <td>0.905800</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>1.100000e+06</td>\n",
       "      <td>1.183690e+06</td>\n",
       "      <td>7.220000e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.009901</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-11-18</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>0.940500</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>6.811000e+05</td>\n",
       "      <td>6.919940e+05</td>\n",
       "      <td>7.360000e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.007323</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-11-19</td>\n",
       "      <td>0.988700</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.955500</td>\n",
       "      <td>4.435000e+05</td>\n",
       "      <td>4.558660e+05</td>\n",
       "      <td>7.280000e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.015908</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3082</th>\n",
       "      <td>2024-05-12</td>\n",
       "      <td>2912.000000</td>\n",
       "      <td>2951.000000</td>\n",
       "      <td>2906.000000</td>\n",
       "      <td>2931.000000</td>\n",
       "      <td>5.200000e+09</td>\n",
       "      <td>1.779026e+06</td>\n",
       "      <td>3.581000e+11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010926</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083</th>\n",
       "      <td>2024-05-13</td>\n",
       "      <td>2916.796334</td>\n",
       "      <td>2984.551884</td>\n",
       "      <td>2868.448720</td>\n",
       "      <td>2948.303455</td>\n",
       "      <td>1.393028e+10</td>\n",
       "      <td>4.735937e+06</td>\n",
       "      <td>3.541556e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.009737</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3084</th>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>2954.422784</td>\n",
       "      <td>2957.396066</td>\n",
       "      <td>2867.874351</td>\n",
       "      <td>2881.795992</td>\n",
       "      <td>1.125873e+10</td>\n",
       "      <td>3.871148e+06</td>\n",
       "      <td>3.463060e+11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.054264</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3085</th>\n",
       "      <td>2024-05-15</td>\n",
       "      <td>2890.012735</td>\n",
       "      <td>3040.591286</td>\n",
       "      <td>2885.457706</td>\n",
       "      <td>3035.758955</td>\n",
       "      <td>1.380971e+10</td>\n",
       "      <td>4.681133e+06</td>\n",
       "      <td>3.652259e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.016662</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3086</th>\n",
       "      <td>2024-05-16</td>\n",
       "      <td>3026.241765</td>\n",
       "      <td>3026.768367</td>\n",
       "      <td>2926.112640</td>\n",
       "      <td>2943.585128</td>\n",
       "      <td>1.103960e+10</td>\n",
       "      <td>3.703648e+06</td>\n",
       "      <td>3.535010e+11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3087 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date         open         high          low        close  \\\n",
       "0    2015-11-15     0.891200     0.921500     0.875000     0.906400   \n",
       "1    2015-11-16     0.906200     0.944700     0.892000     0.929000   \n",
       "2    2015-11-17     0.924900     1.030000     0.905800     1.010000   \n",
       "3    2015-11-18     0.990000     1.010000     0.940500     0.990000   \n",
       "4    2015-11-19     0.988700     1.010000     0.937500     0.955500   \n",
       "...         ...          ...          ...          ...          ...   \n",
       "3082 2024-05-12  2912.000000  2951.000000  2906.000000  2931.000000   \n",
       "3083 2024-05-13  2916.796334  2984.551884  2868.448720  2948.303455   \n",
       "3084 2024-05-14  2954.422784  2957.396066  2867.874351  2881.795992   \n",
       "3085 2024-05-15  2890.012735  3040.591286  2885.457706  3035.758955   \n",
       "3086 2024-05-16  3026.241765  3026.768367  2926.112640  2943.585128   \n",
       "\n",
       "            volume    volume_eth    market_cap  \\\n",
       "0     4.118000e+05  4.583650e+05  6.720000e+07   \n",
       "1     6.209000e+05  6.764420e+05  6.860000e+07   \n",
       "2     1.100000e+06  1.183690e+06  7.220000e+07   \n",
       "3     6.811000e+05  6.919940e+05  7.360000e+07   \n",
       "4     4.435000e+05  4.558660e+05  7.280000e+07   \n",
       "...            ...           ...           ...   \n",
       "3082  5.200000e+09  1.779026e+06  3.581000e+11   \n",
       "3083  1.393028e+10  4.735937e+06  3.541556e+11   \n",
       "3084  1.125873e+10  3.871148e+06  3.463060e+11   \n",
       "3085  1.380971e+10  4.681133e+06  3.652259e+11   \n",
       "3086  1.103960e+10  3.703648e+06  3.535010e+11   \n",
       "\n",
       "      tmw_1_0_percent_increase_binary  tmw_percent_increase_to_avg_high_low  \\\n",
       "0                                 1.0                              0.033594   \n",
       "1                                 1.0                              0.097955   \n",
       "2                                 0.0                             -0.009901   \n",
       "3                                 0.0                             -0.007323   \n",
       "4                                 0.0                             -0.015908   \n",
       "...                               ...                                   ...   \n",
       "3082                              1.0                              0.010926   \n",
       "3083                              0.0                             -0.009737   \n",
       "3084                              1.0                              0.054264   \n",
       "3085                              0.0                             -0.016662   \n",
       "3086                              NaN                                   NaN   \n",
       "\n",
       "      ...  last_100_day_40th_to_50th_pct_spread_count  \\\n",
       "0     ...                                         8.0   \n",
       "1     ...                                         8.0   \n",
       "2     ...                                         8.0   \n",
       "3     ...                                         8.0   \n",
       "4     ...                                         8.0   \n",
       "...   ...                                         ...   \n",
       "3082  ...                                        14.0   \n",
       "3083  ...                                        14.0   \n",
       "3084  ...                                        14.0   \n",
       "3085  ...                                        14.0   \n",
       "3086  ...                                        14.0   \n",
       "\n",
       "      last_100_day_50th_to_60th_pct_spread_count  \\\n",
       "0                                            7.0   \n",
       "1                                            8.0   \n",
       "2                                            8.0   \n",
       "3                                            8.0   \n",
       "4                                            8.0   \n",
       "...                                          ...   \n",
       "3082                                         9.0   \n",
       "3083                                         9.0   \n",
       "3084                                         9.0   \n",
       "3085                                        10.0   \n",
       "3086                                        10.0   \n",
       "\n",
       "      last_100_day_60th_to_70th_pct_spread_count  \\\n",
       "0                                            6.0   \n",
       "1                                            6.0   \n",
       "2                                            6.0   \n",
       "3                                            7.0   \n",
       "4                                            8.0   \n",
       "...                                          ...   \n",
       "3082                                        10.0   \n",
       "3083                                        10.0   \n",
       "3084                                        10.0   \n",
       "3085                                        10.0   \n",
       "3086                                        10.0   \n",
       "\n",
       "      last_100_day_70th_to_80th_pct_spread_count  \\\n",
       "0                                           18.0   \n",
       "1                                           18.0   \n",
       "2                                           18.0   \n",
       "3                                           18.0   \n",
       "4                                           18.0   \n",
       "...                                          ...   \n",
       "3082                                         8.0   \n",
       "3083                                         8.0   \n",
       "3084                                         8.0   \n",
       "3085                                         8.0   \n",
       "3086                                         8.0   \n",
       "\n",
       "      last_100_day_90th_to_95th_pct_spread_count  \\\n",
       "0                                           18.0   \n",
       "1                                           18.0   \n",
       "2                                           18.0   \n",
       "3                                           17.0   \n",
       "4                                           17.0   \n",
       "...                                          ...   \n",
       "3082                                         3.0   \n",
       "3083                                         3.0   \n",
       "3084                                         3.0   \n",
       "3085                                         3.0   \n",
       "3086                                         3.0   \n",
       "\n",
       "      last_100_day_40th_pct_spread_count  last_100_day_50th_pct_spread_count  \\\n",
       "0                                   94.0                                86.0   \n",
       "1                                   94.0                                86.0   \n",
       "2                                   94.0                                86.0   \n",
       "3                                   94.0                                86.0   \n",
       "4                                   94.0                                86.0   \n",
       "...                                  ...                                 ...   \n",
       "3082                                48.0                                34.0   \n",
       "3083                                48.0                                34.0   \n",
       "3084                                48.0                                34.0   \n",
       "3085                                49.0                                35.0   \n",
       "3086                                49.0                                35.0   \n",
       "\n",
       "      last_100_day_60th_pct_spread_count  last_100_day_80th_pct_spread_count  \\\n",
       "0                                   79.0                                55.0   \n",
       "1                                   78.0                                54.0   \n",
       "2                                   78.0                                54.0   \n",
       "3                                   78.0                                53.0   \n",
       "4                                   78.0                                52.0   \n",
       "...                                  ...                                 ...   \n",
       "3082                                25.0                                 7.0   \n",
       "3083                                25.0                                 7.0   \n",
       "3084                                25.0                                 7.0   \n",
       "3085                                25.0                                 7.0   \n",
       "3086                                25.0                                 7.0   \n",
       "\n",
       "      last_100_day_90th_pct_spread_count  \n",
       "0                                   46.0  \n",
       "1                                   45.0  \n",
       "2                                   44.0  \n",
       "3                                   43.0  \n",
       "4                                   42.0  \n",
       "...                                  ...  \n",
       "3082                                 3.0  \n",
       "3083                                 3.0  \n",
       "3084                                 3.0  \n",
       "3085                                 3.0  \n",
       "3086                                 3.0  \n",
       "\n",
       "[3087 rows x 106 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eth_old2 = eth_old.copy()\n",
    "eth_final = pd.concat([eth_old2[:-1], eth_new_days_transformed], ignore_index=True)\n",
    "eth_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data back into Hopsworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection closed.\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "UserWarning: The installed hopsworks client version 3.4.4 may not be compatible with the connected Hopsworks backend version 3.7.1. \n",
      "To ensure compatibility please install the latest bug fix release matching the minor version of your backend (3.7) by running 'pip install hopsworks==3.7.*'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/711829\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 3087/3087 | Elapsed Time: 00:03 | Remaining Time: 00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: eth_ohlc_transformed_2_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai/p/711829/jobs/named/eth_ohlc_transformed_2_offline_fg_materialization/executions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eth_final['tmw_1_0_percent_increase_binary'] = eth_final['tmw_1_0_percent_increase_binary'].astype('Int64')\n",
    "\n",
    "upload_data(eth_final, 'eth_ohlc_transformed', 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_crypto_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
