{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davydsadovskyy/GitBackedShit/crypto-prediction/venv_crypto_prediction/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /Users/davydsadovskyy/GitBackedShit/crypto-prediction/notebooks\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "# from src.load_transform_data import get_new_ethereum_ohlc, get_new_ethereum_ohlc_2\n",
    "from src.hopsworks_connections import pull_data, upload_data, pull_model\n",
    "import pandas_ta as ta\n",
    "import numpy as np\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# Print the current working directory\n",
    "current_working_directory = os.getcwd()\n",
    "print(\"Current Working Directory:\", current_working_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Earlier, I used to scrape data from coinlore.com, but I realized they update it several hours late every day, so now I use coinGecko API instead. This was my code for that..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the last 30 days of Ethereum OHLC data from coinlore.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### This is the scraping function that is used in github actions - it wont work locally unless you have chromeDriver installed #####\n",
    "# But if you want ChromeDriver locally, you can do this (Mac):\n",
    "# 1. brew install chromedriver\n",
    "# 2. xattr -d com.apple.quarantine $(which chromedriver)\n",
    "# 3. which chromedriver\n",
    "# 4. the above line give the path to chromedriver. Now you can go into src/load_transfrom_data.py, and change the path in the get_new_ethereum_ohlc() function.  \n",
    "# new_eth_ohlc = get_new_ethereum_ohlc()\n",
    "\n",
    "##### Use this function when you running locally. This one doesn't require chromeDriver\n",
    "# new_eth_ohlc = get_new_ethereum_ohlc_2()\n",
    "\n",
    "# new_eth_ohlc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean the new raw ethereum data so hopsworks accepts it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_value(value):\n",
    "#     \"\"\"\n",
    "#     Converts a string value to a float. Removes $ signs, and converts\n",
    "#     billion (bn), million (m), and thousand (K) values to their numeric equivalents.\n",
    "#     \"\"\"\n",
    "#     value = value.replace('$', '')  # Remove $ sign to simplify processing\n",
    "#     if value[-1].lower() == 'm':\n",
    "#         return float(value[:-1]) * 1_000_000\n",
    "#     elif value[-1].lower() == 'b':\n",
    "#         return float(value[:-1]) * 1_000_000_000\n",
    "#     elif value[-1].lower() == 'k':\n",
    "#         return float(value[:-1]) * 1_000\n",
    "#     elif value[-2:].lower() == 'bn':  # Handle 'bn' for billions\n",
    "#         return float(value[:-2]) * 1_000_000_000\n",
    "#     else:\n",
    "#         return float(value)\n",
    "\n",
    "# new_eth_ohlc.columns = [col.lower() for col in new_eth_ohlc.columns]\n",
    "# new_eth_ohlc.rename(columns={'volume(eth)': 'volume_eth', 'market cap': 'market_cap'}, inplace=True)\n",
    "\n",
    "# for col in ['open', 'high', 'low', 'close', 'volume', 'market_cap']:\n",
    "#     new_eth_ohlc[col] = new_eth_ohlc[col].apply(convert_value)\n",
    "\n",
    "# new_eth_ohlc['date'] = pd.to_datetime(new_eth_ohlc['date'])\n",
    "\n",
    "# new_eth_ohlc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine this latest raw Etherum data with existing raw Ethereum data in hopsworks, and save it back into hopsworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_eth_ohlc = pull_data('raw_ethereum_ohlc', 1, 'raw_ethereum_ohlc_view', 1)\n",
    "# raw_eth_ohlc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hopsowrks returns fucked date column object, with weird time zone formats, we need to standardize first\n",
    "# raw_eth_ohlc['date'] = pd.to_datetime(raw_eth_ohlc['date']).dt.tz_localize(None)\n",
    "# new_eth_ohlc['date'] = pd.to_datetime(new_eth_ohlc['date']).dt.tz_localize(None)\n",
    "\n",
    "# combined_raw_eth_ohlc = pd.concat([new_eth_ohlc, raw_eth_ohlc], ignore_index=True)\n",
    "# combined_raw_eth_ohlc = combined_raw_eth_ohlc.drop_duplicates(subset='date', keep='last')\n",
    "# combined_raw_eth_ohlc = combined_raw_eth_ohlc.sort_values(by='date')\n",
    "# combined_raw_eth_ohlc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload_data(combined_raw_eth_ohlc, 'raw_ethereum_ohlc', 1, 'Raw ethereum OHLC (open, high, low, close) data from coinlore.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get recent data using CoinGecko API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2024, 5, 14, 19, 12, 55, 400295)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_recent_data(crypto):\n",
    "    # 20:00, or 8pm in EST is 12:00 in UTC, which is the cycle by which crypto OHLC are defined\n",
    "    # end_date = datetime.now().replace(hour=20, minute=0, second=0, microsecond=0)\n",
    "    # ^^^ ONLY WHEN RUNING LOCALLY, on github actions this will retrieve different data, so just use datetime.now() and schedule the run at 8pm EST = 0:00 UTC\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=1)\n",
    "\n",
    "    url = f\"https://api.coingecko.com/api/v3/coins/{crypto}/market_chart/range\"\n",
    "    params = {\n",
    "        'vs_currency': 'usd',\n",
    "        'from': int(start_date.timestamp()),\n",
    "        'to': int(end_date.timestamp())\n",
    "    }\n",
    "    \n",
    "    response = safe_request(url, params)\n",
    "    \n",
    "    if response and response.status_code == 200:\n",
    "        data = response.json()\n",
    "        df = pd.DataFrame(data['prices'], columns=['date', 'price'])\n",
    "        df['date'] = pd.to_datetime(df['date'], unit='ms')\n",
    "        df['market_cap'] = pd.DataFrame(data['market_caps'])[1].values\n",
    "        df['volume_24h'] = pd.DataFrame(data['total_volumes'])[1].values\n",
    "    else:\n",
    "        print(f\"Failed to fetch data. Status code: {response.status_code}\" if response else \"Failed to fetch data; no response.\")\n",
    "    \n",
    "    # Include only the earliest day\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    # Step 2: Extract the date part from the 'datetime' column\n",
    "    df['no_hour_date'] = df['date'].dt.date\n",
    "    earliest_date = df['no_hour_date'].min()\n",
    "    filtered_df = df[df['no_hour_date'] == earliest_date]\n",
    "    filtered_df = filtered_df.drop(columns='no_hour_date')\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "def safe_request(url, params, retries=20, backoff_factor=0.5):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            elif response.status_code == 429:\n",
    "                # We're being rate-limited; back off and retry\n",
    "                sleep_time = backoff_factor * (2 ** i)\n",
    "                print(f\"Rate limit hit. Waiting {sleep_time:.2f} seconds before retrying...\")\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                # Other errors, break the retry loop and return None\n",
    "                print(f\"Request failed with status code {response.status_code}.\")\n",
    "                return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request exception: {e}. Retrying...\")\n",
    "            time.sleep(backoff_factor * (2 ** i))\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>market_cap</th>\n",
       "      <th>volume_24h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-14 02:11:34.254</td>\n",
       "      <td>2945.254691</td>\n",
       "      <td>3.536196e+11</td>\n",
       "      <td>1.390973e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-05-14 02:17:15.569</td>\n",
       "      <td>2943.702108</td>\n",
       "      <td>3.536196e+11</td>\n",
       "      <td>1.392130e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-05-14 02:22:44.720</td>\n",
       "      <td>2943.326684</td>\n",
       "      <td>3.536752e+11</td>\n",
       "      <td>1.394738e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-05-14 02:26:52.257</td>\n",
       "      <td>2944.395684</td>\n",
       "      <td>3.536752e+11</td>\n",
       "      <td>1.394404e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-05-14 02:31:42.672</td>\n",
       "      <td>2944.299962</td>\n",
       "      <td>3.535251e+11</td>\n",
       "      <td>1.390346e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>2024-05-14 23:35:33.686</td>\n",
       "      <td>2881.806758</td>\n",
       "      <td>3.462601e+11</td>\n",
       "      <td>1.117981e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>2024-05-14 23:41:13.076</td>\n",
       "      <td>2882.118003</td>\n",
       "      <td>3.461856e+11</td>\n",
       "      <td>1.112505e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>2024-05-14 23:45:54.287</td>\n",
       "      <td>2881.638415</td>\n",
       "      <td>3.461856e+11</td>\n",
       "      <td>1.123470e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>2024-05-14 23:51:07.643</td>\n",
       "      <td>2883.120296</td>\n",
       "      <td>3.463060e+11</td>\n",
       "      <td>1.125938e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>2024-05-14 23:55:47.419</td>\n",
       "      <td>2881.795992</td>\n",
       "      <td>3.463060e+11</td>\n",
       "      <td>1.125873e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>262 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date        price    market_cap    volume_24h\n",
       "0   2024-05-14 02:11:34.254  2945.254691  3.536196e+11  1.390973e+10\n",
       "1   2024-05-14 02:17:15.569  2943.702108  3.536196e+11  1.392130e+10\n",
       "2   2024-05-14 02:22:44.720  2943.326684  3.536752e+11  1.394738e+10\n",
       "3   2024-05-14 02:26:52.257  2944.395684  3.536752e+11  1.394404e+10\n",
       "4   2024-05-14 02:31:42.672  2944.299962  3.535251e+11  1.390346e+10\n",
       "..                      ...          ...           ...           ...\n",
       "257 2024-05-14 23:35:33.686  2881.806758  3.462601e+11  1.117981e+10\n",
       "258 2024-05-14 23:41:13.076  2882.118003  3.461856e+11  1.112505e+10\n",
       "259 2024-05-14 23:45:54.287  2881.638415  3.461856e+11  1.123470e+10\n",
       "260 2024-05-14 23:51:07.643  2883.120296  3.463060e+11  1.125938e+10\n",
       "261 2024-05-14 23:55:47.419  2881.795992  3.463060e+11  1.125873e+10\n",
       "\n",
       "[262 rows x 4 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eth_raw = fetch_recent_data(\"ethereum\")\n",
    "eth_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform this data into ohlc (open, high, low, close) data for the past day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>volume_eth</th>\n",
       "      <th>market_cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>2945.254691</td>\n",
       "      <td>2952.313133</td>\n",
       "      <td>2867.874351</td>\n",
       "      <td>2881.795992</td>\n",
       "      <td>1.125873e+10</td>\n",
       "      <td>3.873530e+06</td>\n",
       "      <td>3.463060e+11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date         open         high          low        close  \\\n",
       "0 2024-05-14  2945.254691  2952.313133  2867.874351  2881.795992   \n",
       "\n",
       "         volume    volume_eth    market_cap  \n",
       "0  1.125873e+10  3.873530e+06  3.463060e+11  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eth_new = eth_raw.resample('D', on='date').agg({\n",
    "    'price': ['first', 'max', 'min', 'last', 'mean'],\n",
    "    'volume_24h': 'last',\n",
    "    'market_cap': 'last'\n",
    "}).reset_index()\n",
    "eth_new.columns = ['date', 'open', 'high', 'low', 'close', 'avg_price', 'volume', 'market_cap']\n",
    "eth_new['volume_eth'] = eth_new['volume'] / eth_new['avg_price']\n",
    "eth_new = eth_new[['date', 'open', 'high', 'low', 'close', 'volume', 'volume_eth', 'market_cap']]\n",
    "\n",
    "eth_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine this recent data with what we already have in hopsworks feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection closed.\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "UserWarning: The installed hopsworks client version 3.4.4 may not be compatible with the connected Hopsworks backend version 3.7.1. \n",
      "To ensure compatibility please install the latest bug fix release matching the minor version of your backend (3.7) by running 'pip install hopsworks==3.7.*'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/711829\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "Feature view already existed. Skip creation.\n",
      "Finished: Reading data from Hopsworks, using ArrowFlight (1.32s) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VersionWarning: Incremented version to `22`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>volume_eth</th>\n",
       "      <th>market_cap</th>\n",
       "      <th>tmw_1_0_percent_increase_binary</th>\n",
       "      <th>tmw_percent_increase_to_avg_high_low</th>\n",
       "      <th>...</th>\n",
       "      <th>last_100_day_40th_to_50th_pct_spread_count</th>\n",
       "      <th>last_100_day_50th_to_60th_pct_spread_count</th>\n",
       "      <th>last_100_day_60th_to_70th_pct_spread_count</th>\n",
       "      <th>last_100_day_70th_to_80th_pct_spread_count</th>\n",
       "      <th>last_100_day_90th_to_95th_pct_spread_count</th>\n",
       "      <th>last_100_day_40th_pct_spread_count</th>\n",
       "      <th>last_100_day_50th_pct_spread_count</th>\n",
       "      <th>last_100_day_60th_pct_spread_count</th>\n",
       "      <th>last_100_day_80th_pct_spread_count</th>\n",
       "      <th>last_100_day_90th_pct_spread_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-11-15</td>\n",
       "      <td>0.891200</td>\n",
       "      <td>0.921500</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.906400</td>\n",
       "      <td>4.118000e+05</td>\n",
       "      <td>4.583650e+05</td>\n",
       "      <td>6.720000e+07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.033594</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-11-16</td>\n",
       "      <td>0.906200</td>\n",
       "      <td>0.944700</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.929000</td>\n",
       "      <td>6.209000e+05</td>\n",
       "      <td>6.764420e+05</td>\n",
       "      <td>6.860000e+07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.097955</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-11-17</td>\n",
       "      <td>0.924900</td>\n",
       "      <td>1.030000</td>\n",
       "      <td>0.905800</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>1.100000e+06</td>\n",
       "      <td>1.183690e+06</td>\n",
       "      <td>7.220000e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.009901</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-11-18</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>0.940500</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>6.811000e+05</td>\n",
       "      <td>6.919940e+05</td>\n",
       "      <td>7.360000e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.007323</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-11-19</td>\n",
       "      <td>0.988700</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.955500</td>\n",
       "      <td>4.435000e+05</td>\n",
       "      <td>4.558660e+05</td>\n",
       "      <td>7.280000e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.015908</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3080</th>\n",
       "      <td>2024-05-10</td>\n",
       "      <td>3035.000000</td>\n",
       "      <td>3050.000000</td>\n",
       "      <td>2889.000000</td>\n",
       "      <td>2910.000000</td>\n",
       "      <td>1.060000e+10</td>\n",
       "      <td>3.557434e+06</td>\n",
       "      <td>3.650000e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004467</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3081</th>\n",
       "      <td>2024-05-11</td>\n",
       "      <td>2909.000000</td>\n",
       "      <td>2935.000000</td>\n",
       "      <td>2894.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>5.900000e+09</td>\n",
       "      <td>2.033385e+06</td>\n",
       "      <td>3.569000e+11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010306</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3082</th>\n",
       "      <td>2024-05-12</td>\n",
       "      <td>2912.000000</td>\n",
       "      <td>2951.000000</td>\n",
       "      <td>2906.000000</td>\n",
       "      <td>2931.000000</td>\n",
       "      <td>5.200000e+09</td>\n",
       "      <td>1.779026e+06</td>\n",
       "      <td>3.581000e+11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010926</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083</th>\n",
       "      <td>2024-05-13</td>\n",
       "      <td>2916.796334</td>\n",
       "      <td>2984.551884</td>\n",
       "      <td>2868.448720</td>\n",
       "      <td>2948.303455</td>\n",
       "      <td>1.393028e+10</td>\n",
       "      <td>4.735937e+06</td>\n",
       "      <td>3.541556e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.009737</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3084</th>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>2954.422784</td>\n",
       "      <td>2957.396066</td>\n",
       "      <td>2867.874351</td>\n",
       "      <td>2881.795992</td>\n",
       "      <td>1.125873e+10</td>\n",
       "      <td>3.871148e+06</td>\n",
       "      <td>3.463060e+11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3085 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date         open         high          low        close  \\\n",
       "0     2015-11-15     0.891200     0.921500     0.875000     0.906400   \n",
       "1     2015-11-16     0.906200     0.944700     0.892000     0.929000   \n",
       "2     2015-11-17     0.924900     1.030000     0.905800     1.010000   \n",
       "3     2015-11-18     0.990000     1.010000     0.940500     0.990000   \n",
       "4     2015-11-19     0.988700     1.010000     0.937500     0.955500   \n",
       "...          ...          ...          ...          ...          ...   \n",
       "3080  2024-05-10  3035.000000  3050.000000  2889.000000  2910.000000   \n",
       "3081  2024-05-11  2909.000000  2935.000000  2894.000000  2911.000000   \n",
       "3082  2024-05-12  2912.000000  2951.000000  2906.000000  2931.000000   \n",
       "3083  2024-05-13  2916.796334  2984.551884  2868.448720  2948.303455   \n",
       "3084  2024-05-14  2954.422784  2957.396066  2867.874351  2881.795992   \n",
       "\n",
       "            volume    volume_eth    market_cap  \\\n",
       "0     4.118000e+05  4.583650e+05  6.720000e+07   \n",
       "1     6.209000e+05  6.764420e+05  6.860000e+07   \n",
       "2     1.100000e+06  1.183690e+06  7.220000e+07   \n",
       "3     6.811000e+05  6.919940e+05  7.360000e+07   \n",
       "4     4.435000e+05  4.558660e+05  7.280000e+07   \n",
       "...            ...           ...           ...   \n",
       "3080  1.060000e+10  3.557434e+06  3.650000e+11   \n",
       "3081  5.900000e+09  2.033385e+06  3.569000e+11   \n",
       "3082  5.200000e+09  1.779026e+06  3.581000e+11   \n",
       "3083  1.393028e+10  4.735937e+06  3.541556e+11   \n",
       "3084  1.125873e+10  3.871148e+06  3.463060e+11   \n",
       "\n",
       "      tmw_1_0_percent_increase_binary  tmw_percent_increase_to_avg_high_low  \\\n",
       "0                                 1.0                              0.033594   \n",
       "1                                 1.0                              0.097955   \n",
       "2                                 0.0                             -0.009901   \n",
       "3                                 0.0                             -0.007323   \n",
       "4                                 0.0                             -0.015908   \n",
       "...                               ...                                   ...   \n",
       "3080                              0.0                              0.004467   \n",
       "3081                              1.0                              0.010306   \n",
       "3082                              1.0                              0.010926   \n",
       "3083                              0.0                             -0.009737   \n",
       "3084                              NaN                                   NaN   \n",
       "\n",
       "      ...  last_100_day_40th_to_50th_pct_spread_count  \\\n",
       "0     ...                                         8.0   \n",
       "1     ...                                         8.0   \n",
       "2     ...                                         8.0   \n",
       "3     ...                                         8.0   \n",
       "4     ...                                         8.0   \n",
       "...   ...                                         ...   \n",
       "3080  ...                                        14.0   \n",
       "3081  ...                                        14.0   \n",
       "3082  ...                                        14.0   \n",
       "3083  ...                                        14.0   \n",
       "3084  ...                                        14.0   \n",
       "\n",
       "      last_100_day_50th_to_60th_pct_spread_count  \\\n",
       "0                                            7.0   \n",
       "1                                            8.0   \n",
       "2                                            8.0   \n",
       "3                                            8.0   \n",
       "4                                            8.0   \n",
       "...                                          ...   \n",
       "3080                                         9.0   \n",
       "3081                                         9.0   \n",
       "3082                                         9.0   \n",
       "3083                                         9.0   \n",
       "3084                                         9.0   \n",
       "\n",
       "      last_100_day_60th_to_70th_pct_spread_count  \\\n",
       "0                                            6.0   \n",
       "1                                            6.0   \n",
       "2                                            6.0   \n",
       "3                                            7.0   \n",
       "4                                            8.0   \n",
       "...                                          ...   \n",
       "3080                                        10.0   \n",
       "3081                                        10.0   \n",
       "3082                                        10.0   \n",
       "3083                                        10.0   \n",
       "3084                                        10.0   \n",
       "\n",
       "      last_100_day_70th_to_80th_pct_spread_count  \\\n",
       "0                                           18.0   \n",
       "1                                           18.0   \n",
       "2                                           18.0   \n",
       "3                                           18.0   \n",
       "4                                           18.0   \n",
       "...                                          ...   \n",
       "3080                                         8.0   \n",
       "3081                                         8.0   \n",
       "3082                                         8.0   \n",
       "3083                                         8.0   \n",
       "3084                                         8.0   \n",
       "\n",
       "      last_100_day_90th_to_95th_pct_spread_count  \\\n",
       "0                                           18.0   \n",
       "1                                           18.0   \n",
       "2                                           18.0   \n",
       "3                                           17.0   \n",
       "4                                           17.0   \n",
       "...                                          ...   \n",
       "3080                                         3.0   \n",
       "3081                                         3.0   \n",
       "3082                                         3.0   \n",
       "3083                                         3.0   \n",
       "3084                                         3.0   \n",
       "\n",
       "      last_100_day_40th_pct_spread_count  last_100_day_50th_pct_spread_count  \\\n",
       "0                                   94.0                                86.0   \n",
       "1                                   94.0                                86.0   \n",
       "2                                   94.0                                86.0   \n",
       "3                                   94.0                                86.0   \n",
       "4                                   94.0                                86.0   \n",
       "...                                  ...                                 ...   \n",
       "3080                                48.0                                34.0   \n",
       "3081                                48.0                                34.0   \n",
       "3082                                48.0                                34.0   \n",
       "3083                                48.0                                34.0   \n",
       "3084                                48.0                                34.0   \n",
       "\n",
       "      last_100_day_60th_pct_spread_count  last_100_day_80th_pct_spread_count  \\\n",
       "0                                   79.0                                55.0   \n",
       "1                                   78.0                                54.0   \n",
       "2                                   78.0                                54.0   \n",
       "3                                   78.0                                53.0   \n",
       "4                                   78.0                                52.0   \n",
       "...                                  ...                                 ...   \n",
       "3080                                25.0                                 7.0   \n",
       "3081                                25.0                                 7.0   \n",
       "3082                                25.0                                 7.0   \n",
       "3083                                25.0                                 7.0   \n",
       "3084                                25.0                                 7.0   \n",
       "\n",
       "      last_100_day_90th_pct_spread_count  \n",
       "0                                   46.0  \n",
       "1                                   45.0  \n",
       "2                                   44.0  \n",
       "3                                   43.0  \n",
       "4                                   42.0  \n",
       "...                                  ...  \n",
       "3080                                 3.0  \n",
       "3081                                 3.0  \n",
       "3082                                 3.0  \n",
       "3083                                 3.0  \n",
       "3084                                 3.0  \n",
       "\n",
       "[3085 rows x 106 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eth_old = pull_data('eth_ohlc_transformed', 2, 'eth_ohlc_transformed_view', 2)\n",
    "eth_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>volume_eth</th>\n",
       "      <th>market_cap</th>\n",
       "      <th>tmw_1_0_percent_increase_binary</th>\n",
       "      <th>tmw_percent_increase_to_avg_high_low</th>\n",
       "      <th>...</th>\n",
       "      <th>last_100_day_40th_to_50th_pct_spread_count</th>\n",
       "      <th>last_100_day_50th_to_60th_pct_spread_count</th>\n",
       "      <th>last_100_day_60th_to_70th_pct_spread_count</th>\n",
       "      <th>last_100_day_70th_to_80th_pct_spread_count</th>\n",
       "      <th>last_100_day_90th_to_95th_pct_spread_count</th>\n",
       "      <th>last_100_day_40th_pct_spread_count</th>\n",
       "      <th>last_100_day_50th_pct_spread_count</th>\n",
       "      <th>last_100_day_60th_pct_spread_count</th>\n",
       "      <th>last_100_day_80th_pct_spread_count</th>\n",
       "      <th>last_100_day_90th_pct_spread_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-11-15</td>\n",
       "      <td>0.891200</td>\n",
       "      <td>0.921500</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>0.906400</td>\n",
       "      <td>4.118000e+05</td>\n",
       "      <td>4.583650e+05</td>\n",
       "      <td>6.720000e+07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.033594</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-11-16</td>\n",
       "      <td>0.906200</td>\n",
       "      <td>0.944700</td>\n",
       "      <td>0.89200</td>\n",
       "      <td>0.929000</td>\n",
       "      <td>6.209000e+05</td>\n",
       "      <td>6.764420e+05</td>\n",
       "      <td>6.860000e+07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.097955</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-11-17</td>\n",
       "      <td>0.924900</td>\n",
       "      <td>1.030000</td>\n",
       "      <td>0.90580</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>1.100000e+06</td>\n",
       "      <td>1.183690e+06</td>\n",
       "      <td>7.220000e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.009901</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-11-18</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>0.94050</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>6.811000e+05</td>\n",
       "      <td>6.919940e+05</td>\n",
       "      <td>7.360000e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.007323</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-11-19</td>\n",
       "      <td>0.988700</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>0.93750</td>\n",
       "      <td>0.955500</td>\n",
       "      <td>4.435000e+05</td>\n",
       "      <td>4.558660e+05</td>\n",
       "      <td>7.280000e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.015908</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3079</th>\n",
       "      <td>2024-05-09</td>\n",
       "      <td>2977.000000</td>\n",
       "      <td>3055.000000</td>\n",
       "      <td>2958.00000</td>\n",
       "      <td>3036.000000</td>\n",
       "      <td>9.600000e+09</td>\n",
       "      <td>3.190231e+06</td>\n",
       "      <td>3.670000e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.018445</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3080</th>\n",
       "      <td>2024-05-10</td>\n",
       "      <td>3035.000000</td>\n",
       "      <td>3050.000000</td>\n",
       "      <td>2889.00000</td>\n",
       "      <td>2910.000000</td>\n",
       "      <td>1.060000e+10</td>\n",
       "      <td>3.557434e+06</td>\n",
       "      <td>3.650000e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004467</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3081</th>\n",
       "      <td>2024-05-11</td>\n",
       "      <td>2909.000000</td>\n",
       "      <td>2935.000000</td>\n",
       "      <td>2894.00000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>5.900000e+09</td>\n",
       "      <td>2.033385e+06</td>\n",
       "      <td>3.569000e+11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010306</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3082</th>\n",
       "      <td>2024-05-12</td>\n",
       "      <td>2912.000000</td>\n",
       "      <td>2951.000000</td>\n",
       "      <td>2906.00000</td>\n",
       "      <td>2931.000000</td>\n",
       "      <td>5.200000e+09</td>\n",
       "      <td>1.779026e+06</td>\n",
       "      <td>3.581000e+11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010926</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083</th>\n",
       "      <td>2024-05-13</td>\n",
       "      <td>2916.796334</td>\n",
       "      <td>2984.551884</td>\n",
       "      <td>2868.44872</td>\n",
       "      <td>2948.303455</td>\n",
       "      <td>1.393028e+10</td>\n",
       "      <td>4.735937e+06</td>\n",
       "      <td>3.541556e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.009737</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3084 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date         open         high         low        close  \\\n",
       "0     2015-11-15     0.891200     0.921500     0.87500     0.906400   \n",
       "1     2015-11-16     0.906200     0.944700     0.89200     0.929000   \n",
       "2     2015-11-17     0.924900     1.030000     0.90580     1.010000   \n",
       "3     2015-11-18     0.990000     1.010000     0.94050     0.990000   \n",
       "4     2015-11-19     0.988700     1.010000     0.93750     0.955500   \n",
       "...          ...          ...          ...         ...          ...   \n",
       "3079  2024-05-09  2977.000000  3055.000000  2958.00000  3036.000000   \n",
       "3080  2024-05-10  3035.000000  3050.000000  2889.00000  2910.000000   \n",
       "3081  2024-05-11  2909.000000  2935.000000  2894.00000  2911.000000   \n",
       "3082  2024-05-12  2912.000000  2951.000000  2906.00000  2931.000000   \n",
       "3083  2024-05-13  2916.796334  2984.551884  2868.44872  2948.303455   \n",
       "\n",
       "            volume    volume_eth    market_cap  \\\n",
       "0     4.118000e+05  4.583650e+05  6.720000e+07   \n",
       "1     6.209000e+05  6.764420e+05  6.860000e+07   \n",
       "2     1.100000e+06  1.183690e+06  7.220000e+07   \n",
       "3     6.811000e+05  6.919940e+05  7.360000e+07   \n",
       "4     4.435000e+05  4.558660e+05  7.280000e+07   \n",
       "...            ...           ...           ...   \n",
       "3079  9.600000e+09  3.190231e+06  3.670000e+11   \n",
       "3080  1.060000e+10  3.557434e+06  3.650000e+11   \n",
       "3081  5.900000e+09  2.033385e+06  3.569000e+11   \n",
       "3082  5.200000e+09  1.779026e+06  3.581000e+11   \n",
       "3083  1.393028e+10  4.735937e+06  3.541556e+11   \n",
       "\n",
       "      tmw_1_0_percent_increase_binary  tmw_percent_increase_to_avg_high_low  \\\n",
       "0                                 1.0                              0.033594   \n",
       "1                                 1.0                              0.097955   \n",
       "2                                 0.0                             -0.009901   \n",
       "3                                 0.0                             -0.007323   \n",
       "4                                 0.0                             -0.015908   \n",
       "...                               ...                                   ...   \n",
       "3079                              0.0                             -0.018445   \n",
       "3080                              0.0                              0.004467   \n",
       "3081                              1.0                              0.010306   \n",
       "3082                              1.0                              0.010926   \n",
       "3083                              0.0                             -0.009737   \n",
       "\n",
       "      ...  last_100_day_40th_to_50th_pct_spread_count  \\\n",
       "0     ...                                         8.0   \n",
       "1     ...                                         8.0   \n",
       "2     ...                                         8.0   \n",
       "3     ...                                         8.0   \n",
       "4     ...                                         8.0   \n",
       "...   ...                                         ...   \n",
       "3079  ...                                        14.0   \n",
       "3080  ...                                        14.0   \n",
       "3081  ...                                        14.0   \n",
       "3082  ...                                        14.0   \n",
       "3083  ...                                        14.0   \n",
       "\n",
       "      last_100_day_50th_to_60th_pct_spread_count  \\\n",
       "0                                            7.0   \n",
       "1                                            8.0   \n",
       "2                                            8.0   \n",
       "3                                            8.0   \n",
       "4                                            8.0   \n",
       "...                                          ...   \n",
       "3079                                         8.0   \n",
       "3080                                         9.0   \n",
       "3081                                         9.0   \n",
       "3082                                         9.0   \n",
       "3083                                         9.0   \n",
       "\n",
       "      last_100_day_60th_to_70th_pct_spread_count  \\\n",
       "0                                            6.0   \n",
       "1                                            6.0   \n",
       "2                                            6.0   \n",
       "3                                            7.0   \n",
       "4                                            8.0   \n",
       "...                                          ...   \n",
       "3079                                        10.0   \n",
       "3080                                        10.0   \n",
       "3081                                        10.0   \n",
       "3082                                        10.0   \n",
       "3083                                        10.0   \n",
       "\n",
       "      last_100_day_70th_to_80th_pct_spread_count  \\\n",
       "0                                           18.0   \n",
       "1                                           18.0   \n",
       "2                                           18.0   \n",
       "3                                           18.0   \n",
       "4                                           18.0   \n",
       "...                                          ...   \n",
       "3079                                         8.0   \n",
       "3080                                         8.0   \n",
       "3081                                         8.0   \n",
       "3082                                         8.0   \n",
       "3083                                         8.0   \n",
       "\n",
       "      last_100_day_90th_to_95th_pct_spread_count  \\\n",
       "0                                           18.0   \n",
       "1                                           18.0   \n",
       "2                                           18.0   \n",
       "3                                           17.0   \n",
       "4                                           17.0   \n",
       "...                                          ...   \n",
       "3079                                         3.0   \n",
       "3080                                         3.0   \n",
       "3081                                         3.0   \n",
       "3082                                         3.0   \n",
       "3083                                         3.0   \n",
       "\n",
       "      last_100_day_40th_pct_spread_count  last_100_day_50th_pct_spread_count  \\\n",
       "0                                   94.0                                86.0   \n",
       "1                                   94.0                                86.0   \n",
       "2                                   94.0                                86.0   \n",
       "3                                   94.0                                86.0   \n",
       "4                                   94.0                                86.0   \n",
       "...                                  ...                                 ...   \n",
       "3079                                47.0                                33.0   \n",
       "3080                                48.0                                34.0   \n",
       "3081                                48.0                                34.0   \n",
       "3082                                48.0                                34.0   \n",
       "3083                                48.0                                34.0   \n",
       "\n",
       "      last_100_day_60th_pct_spread_count  last_100_day_80th_pct_spread_count  \\\n",
       "0                                   79.0                                55.0   \n",
       "1                                   78.0                                54.0   \n",
       "2                                   78.0                                54.0   \n",
       "3                                   78.0                                53.0   \n",
       "4                                   78.0                                52.0   \n",
       "...                                  ...                                 ...   \n",
       "3079                                25.0                                 7.0   \n",
       "3080                                25.0                                 7.0   \n",
       "3081                                25.0                                 7.0   \n",
       "3082                                25.0                                 7.0   \n",
       "3083                                25.0                                 7.0   \n",
       "\n",
       "      last_100_day_90th_pct_spread_count  \n",
       "0                                   46.0  \n",
       "1                                   45.0  \n",
       "2                                   44.0  \n",
       "3                                   43.0  \n",
       "4                                   42.0  \n",
       "...                                  ...  \n",
       "3079                                 3.0  \n",
       "3080                                 3.0  \n",
       "3081                                 3.0  \n",
       "3082                                 3.0  \n",
       "3083                                 3.0  \n",
       "\n",
       "[3084 rows x 106 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eth_old = eth_old[:-1]\n",
    "eth_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the raw, untransformed portion of the old date with the new data we just got from CoinGecko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>volume_eth</th>\n",
       "      <th>market_cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-11-15</td>\n",
       "      <td>0.891200</td>\n",
       "      <td>0.921500</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.906400</td>\n",
       "      <td>4.118000e+05</td>\n",
       "      <td>4.583650e+05</td>\n",
       "      <td>6.720000e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-11-16</td>\n",
       "      <td>0.906200</td>\n",
       "      <td>0.944700</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.929000</td>\n",
       "      <td>6.209000e+05</td>\n",
       "      <td>6.764420e+05</td>\n",
       "      <td>6.860000e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-11-17</td>\n",
       "      <td>0.924900</td>\n",
       "      <td>1.030000</td>\n",
       "      <td>0.905800</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>1.100000e+06</td>\n",
       "      <td>1.183690e+06</td>\n",
       "      <td>7.220000e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-11-18</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>0.940500</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>6.811000e+05</td>\n",
       "      <td>6.919940e+05</td>\n",
       "      <td>7.360000e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-11-19</td>\n",
       "      <td>0.988700</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.955500</td>\n",
       "      <td>4.435000e+05</td>\n",
       "      <td>4.558660e+05</td>\n",
       "      <td>7.280000e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3080</th>\n",
       "      <td>2024-05-10</td>\n",
       "      <td>3035.000000</td>\n",
       "      <td>3050.000000</td>\n",
       "      <td>2889.000000</td>\n",
       "      <td>2910.000000</td>\n",
       "      <td>1.060000e+10</td>\n",
       "      <td>3.557434e+06</td>\n",
       "      <td>3.650000e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3081</th>\n",
       "      <td>2024-05-11</td>\n",
       "      <td>2909.000000</td>\n",
       "      <td>2935.000000</td>\n",
       "      <td>2894.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>5.900000e+09</td>\n",
       "      <td>2.033385e+06</td>\n",
       "      <td>3.569000e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3082</th>\n",
       "      <td>2024-05-12</td>\n",
       "      <td>2912.000000</td>\n",
       "      <td>2951.000000</td>\n",
       "      <td>2906.000000</td>\n",
       "      <td>2931.000000</td>\n",
       "      <td>5.200000e+09</td>\n",
       "      <td>1.779026e+06</td>\n",
       "      <td>3.581000e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083</th>\n",
       "      <td>2024-05-13</td>\n",
       "      <td>2916.796334</td>\n",
       "      <td>2984.551884</td>\n",
       "      <td>2868.448720</td>\n",
       "      <td>2948.303455</td>\n",
       "      <td>1.393028e+10</td>\n",
       "      <td>4.735937e+06</td>\n",
       "      <td>3.541556e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3084</th>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>2945.254691</td>\n",
       "      <td>2952.313133</td>\n",
       "      <td>2867.874351</td>\n",
       "      <td>2881.795992</td>\n",
       "      <td>1.125873e+10</td>\n",
       "      <td>3.873530e+06</td>\n",
       "      <td>3.463060e+11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3085 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date         open         high          low        close  \\\n",
       "0    2015-11-15     0.891200     0.921500     0.875000     0.906400   \n",
       "1    2015-11-16     0.906200     0.944700     0.892000     0.929000   \n",
       "2    2015-11-17     0.924900     1.030000     0.905800     1.010000   \n",
       "3    2015-11-18     0.990000     1.010000     0.940500     0.990000   \n",
       "4    2015-11-19     0.988700     1.010000     0.937500     0.955500   \n",
       "...         ...          ...          ...          ...          ...   \n",
       "3080 2024-05-10  3035.000000  3050.000000  2889.000000  2910.000000   \n",
       "3081 2024-05-11  2909.000000  2935.000000  2894.000000  2911.000000   \n",
       "3082 2024-05-12  2912.000000  2951.000000  2906.000000  2931.000000   \n",
       "3083 2024-05-13  2916.796334  2984.551884  2868.448720  2948.303455   \n",
       "3084 2024-05-14  2945.254691  2952.313133  2867.874351  2881.795992   \n",
       "\n",
       "            volume    volume_eth    market_cap  \n",
       "0     4.118000e+05  4.583650e+05  6.720000e+07  \n",
       "1     6.209000e+05  6.764420e+05  6.860000e+07  \n",
       "2     1.100000e+06  1.183690e+06  7.220000e+07  \n",
       "3     6.811000e+05  6.919940e+05  7.360000e+07  \n",
       "4     4.435000e+05  4.558660e+05  7.280000e+07  \n",
       "...            ...           ...           ...  \n",
       "3080  1.060000e+10  3.557434e+06  3.650000e+11  \n",
       "3081  5.900000e+09  2.033385e+06  3.569000e+11  \n",
       "3082  5.200000e+09  1.779026e+06  3.581000e+11  \n",
       "3083  1.393028e+10  4.735937e+06  3.541556e+11  \n",
       "3084  1.125873e+10  3.873530e+06  3.463060e+11  \n",
       "\n",
       "[3085 rows x 8 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_needed = ['date', 'open', 'high', 'low', 'close', 'volume', 'volume_eth', 'market_cap']\n",
    "\n",
    "# Convert date columns to datetime objects, in case they are not already\n",
    "eth_old['date'] = pd.to_datetime(eth_old['date'])\n",
    "eth_new['date'] = pd.to_datetime(eth_new['date'])\n",
    "\n",
    "# Remove any timezone information to make the comparison straightforward\n",
    "eth_old['date'] = eth_old['date'].dt.tz_localize(None)\n",
    "eth_new['date'] = eth_new['date'].dt.tz_localize(None)\n",
    "\n",
    "if not any(eth_new['date'].isin(eth_old['date'])):\n",
    "    # Since eth_new's date isn't found, just concatenate it\n",
    "    eth_combined = pd.concat([eth_old[columns_needed], eth_new], ignore_index=True)\n",
    "else:\n",
    "    # If the date is found, remove the last row from eth_old and add on the new row\n",
    "    eth_combined = pd.concat([eth_old.iloc[:-1], eth_new], ignore_index=True)\n",
    "\n",
    "eth_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this combined raw dataset to regenerate all the response and predictor variables for the past observations, and for the new observation that was just added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_response_vars(data):\n",
    "    df = data.copy()\n",
    "    tmw_avg_high_close = (df[\"high\"].shift(-1) + df[\"close\"].shift(-1)) / 2\n",
    "    df['tmw_percent_increase_to_avg_high_low'] = ((tmw_avg_high_close - df['close']) / df['close'])\n",
    "    df['tmw_1_0_percent_increase_binary'] = (df['tmw_percent_increase_to_avg_high_low'] >= .01).astype(int)\n",
    "\n",
    "    # Last row doesn't have a value for tomorrow's return, but it was assigned 0's for response variable. Convert them to NA\n",
    "    df.loc[df.index[-1], ['tmw_percent_increase_to_avg_high_low', 'tmw_1_0_percent_increase_binary']] = pd.NA\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_predictor_vars(data):\n",
    "    df = data.copy()\n",
    "\n",
    "    # define 'helper' columns\n",
    "    perc_from_low_to_high = (df['high'] - df['low']) / df['low']\n",
    "\n",
    "    tmw_vol = df[\"volume\"].shift(-1)\n",
    "    tmw_perc_change_vol = (tmw_vol - df['volume']) / df['volume']\n",
    "\n",
    "\n",
    "    # Create binary columns for each month (1-12)\n",
    "    month_names = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december']\n",
    "    for i in range(1, 13):  # Months are typically 1-12\n",
    "        month_name = month_names[i-1]  # Get the month name from the list\n",
    "        df[f'{month_name}'] = (df['date'].dt.month == i).astype(int)\n",
    "\n",
    "    # Create binary columns for each day\n",
    "    days = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "    for i, day in enumerate(days):\n",
    "        df[f'{day.lower()}'] = (df['date'].dt.dayofweek == i).astype(int)\n",
    "\n",
    "    horizons = [2,5,10,25,50,100] \n",
    "    for horizon in horizons:\n",
    "\n",
    "        # Ratio - Close Price Simple Moving Average (SMA)\n",
    "        sma_col = f\"sma_{horizon}\"\n",
    "        df[sma_col] = df[\"close\"] / ta.sma(df[\"close\"], length=horizon)\n",
    "        # Close Price Relative Strength Index (RSI)\n",
    "        rsi_col = f\"rsi_{horizon}\"\n",
    "        df[rsi_col] = ta.rsi(df[\"close\"], length=horizon)\n",
    "\n",
    "        # Ratio - Volume Simple Moving Average (SMA)\n",
    "        sma_col = f\"volume_sma_{horizon}\"\n",
    "        df[sma_col] = df[\"volume\"] / ta.sma(df[\"volume\"], length=horizon)\n",
    "        # Volume Relative Strength Index (RSI)\n",
    "        rsi_col = f\"volume_rsi_{horizon}\"\n",
    "        df[rsi_col] = ta.rsi(df[\"volume\"], length=horizon)\n",
    "\n",
    "        # Ratio - High-Low Spread Simple Moving Average (SMA)\n",
    "        sma_col = f\"spread_sma_{horizon}\"\n",
    "        df[sma_col] = perc_from_low_to_high / ta.sma(perc_from_low_to_high, length=horizon)\n",
    "        # High-Low Spread Relative Strength Index (RSI)\n",
    "        rsi_col = f\"spread_rsi_{horizon}\"\n",
    "        df[rsi_col] = ta.rsi(perc_from_low_to_high, length=horizon)\n",
    "\n",
    "        # Sum of the number of days in the past horizon that had a certain percent change in price from a day's close to the next day's (high+close)/2\n",
    "        df[f\"last_{horizon}_day_40th_to_50th_pct_price_change_count\"] = ((df['tmw_percent_increase_to_avg_high_low'] > 0.002684) & (df['tmw_percent_increase_to_avg_high_low'] <= 0.008635)).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_50th_to_60th_pct_price_change_count\"] = ((df['tmw_percent_increase_to_avg_high_low'] > 0.008635) & (df['tmw_percent_increase_to_avg_high_low'] <= 0.016393)).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_60th_to_70th_pct_price_change_count\"] = ((df['tmw_percent_increase_to_avg_high_low'] > 0.016393) & (df['tmw_percent_increase_to_avg_high_low'] <= 0.026398)).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_70th_to_80th_pct_price_change_count\"] = ((df['tmw_percent_increase_to_avg_high_low'] > 0.026398) & (df['tmw_percent_increase_to_avg_high_low'] <= 0.041709)).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_80th_to_90th_pct_price_change_count\"] = ((df['tmw_percent_increase_to_avg_high_low'] > 0.041709) & (df['tmw_percent_increase_to_avg_high_low'] <= 0.070177)).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_90th_to_95th_pct_price_change_count\"] = ((df['tmw_percent_increase_to_avg_high_low'] > 0.070177) & (df['tmw_percent_increase_to_avg_high_low'] <= 0.106177)).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_40th_pct_price_change_count\"] = (df['tmw_percent_increase_to_avg_high_low'] > 0.002684).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_50th_pct_price_change_count\"] = (df['tmw_percent_increase_to_avg_high_low'] > 0.008635).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_60th_pct_price_change_count\"] = (df['tmw_percent_increase_to_avg_high_low'] > 0.016393).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_70th_pct_price_change_count\"] = (df['tmw_percent_increase_to_avg_high_low'] > 0.026398).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_80th_pct_price_change_count\"] = (df['tmw_percent_increase_to_avg_high_low'] > 0.041709).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_90th_pct_price_change_count\"] = (df['tmw_percent_increase_to_avg_high_low'] > 0.070177).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_95th_pct_price_change_count\"] = (df['tmw_percent_increase_to_avg_high_low'] > 0.106177).astype(int).shift(1).rolling(horizon).sum()\n",
    "                \n",
    "        \n",
    "        # Sum of the number of days in the past horizon that had a certain percent chamge in volume from one day to the next\n",
    "        tmw_vol = df[\"volume\"].shift(-1)\n",
    "        tmw_perc_change_vol = (tmw_vol - df['volume']) / df['volume']\n",
    "        df[f\"last_{horizon}_day_40th_to_50th_pct_volume_change_count\"] = ((tmw_perc_change_vol > -0.069444) & (tmw_perc_change_vol <= -0.008197)).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_50th_to_60th_pct_volume_change_count\"] = ((tmw_perc_change_vol > -0.008197) & (tmw_perc_change_vol <= 0.061069)).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_60th_to_70th_pct_volume_change_count\"] = ((tmw_perc_change_vol > 0.061069) & (tmw_perc_change_vol <= 0.137500)).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_70th_to_80th_pct_volume_change_count\"] = ((tmw_perc_change_vol > 0.137500) & (tmw_perc_change_vol <= 0.276316)).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_80th_to_90th_pct_volume_change_count\"] = ((tmw_perc_change_vol > 0.276316) & (tmw_perc_change_vol <= 0.544118)).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_90th_to_95th_pct_volume_change_count\"] = ((tmw_perc_change_vol > 0.544118) & (tmw_perc_change_vol <= 0.885965)).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_40th_pct_volume_change_count\"] = (tmw_perc_change_vol > -0.069444).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_50th_pct_volume_change_count\"] = (tmw_perc_change_vol > -0.008197).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_60th_pct_volume_change_count\"] = (tmw_perc_change_vol > 0.061069).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_70th_pct_volume_change_count\"] = (tmw_perc_change_vol > 0.137500).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_80th_pct_volume_change_count\"] = (tmw_perc_change_vol > 0.276316).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_90th_pct_volume_change_count\"] = (tmw_perc_change_vol > 0.544118).astype(int).shift(1).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_95th_pct_volume_change_count\"] = (tmw_perc_change_vol > 0.885965).astype(int).shift(1).rolling(horizon).sum()\n",
    "\n",
    "        # Sum of the number of days in the past horizon that had a certain percent chamge in the difference between low and high price\n",
    "        prc_from_low_to_high = (df['high'] - df['low']) / df['low']\n",
    "        df[f\"last_{horizon}_day_40th_to_50th_pct_spread_count\"] = ((prc_from_low_to_high > 0.043674) & (prc_from_low_to_high <= 0.053125)).astype(int).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_50th_to_60th_pct_spread_count\"] = ((prc_from_low_to_high > 0.053125) & (prc_from_low_to_high <= 0.064163)).astype(int).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_60th_to_70th_pct_spread_count\"] = ((prc_from_low_to_high > 0.064163) & (prc_from_low_to_high <= 0.077524)).astype(int).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_70th_to_80th_pct_spread_count\"] = ((prc_from_low_to_high > 0.077524) & (prc_from_low_to_high <= 0.097901)).astype(int).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_80th_to_90th_pct_spread_count\"] = ((prc_from_low_to_high > 0.097901) & (prc_from_low_to_high <= 0.137338)).astype(int).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_90th_to_95th_pct_spread_count\"] = ((prc_from_low_to_high > 0.137338) & (prc_from_low_to_high <= 0.191936)).astype(int).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_40th_pct_spread_count\"] = (prc_from_low_to_high > 0.043674).astype(int).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_50th_pct_spread_count\"] = (prc_from_low_to_high > 0.053125).astype(int).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_60th_pct_spread_count\"] = (prc_from_low_to_high > 0.064163).astype(int).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_70th_pct_spread_count\"] = (prc_from_low_to_high > 0.077524).astype(int).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_80th_pct_spread_count\"] = (prc_from_low_to_high > 0.097901).astype(int).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_90th_pct_spread_count\"] = (prc_from_low_to_high > 0.137338).astype(int).rolling(horizon).sum()\n",
    "        df[f\"last_{horizon}_day_95th_pct_spread_count\"] = (prc_from_low_to_high > 0.191936).astype(int).rolling(horizon).sum()\n",
    "\n",
    "    # only keep the 96 predictor set, the one we found to be the best\n",
    "    predictors_96 = ['rsi_2', 'volume_rsi_2', 'spread_rsi_2', 'last_2_day_80th_pct_volume_change_count', 'last_2_day_60th_to_70th_pct_spread_count', 'last_2_day_40th_pct_spread_count', 'rsi_5', 'volume_rsi_5', 'spread_rsi_5', 'last_5_day_60th_pct_price_change_count', 'last_5_day_70th_pct_price_change_count', 'rsi_10', 'volume_rsi_10', 'spread_rsi_10', 'last_10_day_40th_to_50th_pct_price_change_count', 'last_10_day_60th_pct_price_change_count', 'last_10_day_70th_to_80th_pct_volume_change_count', 'last_10_day_80th_to_90th_pct_spread_count', 'last_10_day_40th_pct_spread_count', 'last_10_day_50th_pct_spread_count', 'last_10_day_70th_pct_spread_count', 'last_10_day_80th_pct_spread_count', 'rsi_25', 'volume_rsi_25', 'spread_rsi_25', 'last_25_day_40th_to_50th_pct_price_change_count', 'last_25_day_70th_to_80th_pct_price_change_count', 'last_25_day_80th_to_90th_pct_price_change_count', 'last_25_day_40th_pct_price_change_count', 'last_25_day_50th_pct_price_change_count', 'last_25_day_80th_pct_price_change_count', 'last_25_day_40th_to_50th_pct_volume_change_count', 'last_25_day_50th_to_60th_pct_volume_change_count', 'last_25_day_60th_to_70th_pct_volume_change_count', 'last_25_day_70th_to_80th_pct_volume_change_count', 'last_25_day_80th_to_90th_pct_volume_change_count', 'last_25_day_50th_pct_volume_change_count', 'last_25_day_60th_pct_volume_change_count', 'last_25_day_70th_pct_volume_change_count', 'last_25_day_80th_pct_volume_change_count', 'last_25_day_40th_to_50th_pct_spread_count', 'last_25_day_50th_to_60th_pct_spread_count', 'last_25_day_60th_to_70th_pct_spread_count', 'last_25_day_70th_to_80th_pct_spread_count', 'last_25_day_80th_to_90th_pct_spread_count', 'last_25_day_40th_pct_spread_count', 'last_25_day_60th_pct_spread_count', 'last_25_day_70th_pct_spread_count', 'last_25_day_90th_pct_spread_count', 'rsi_50', 'last_50_day_50th_to_60th_pct_price_change_count', 'last_50_day_60th_to_70th_pct_price_change_count', 'last_50_day_70th_to_80th_pct_price_change_count', 'last_50_day_40th_pct_price_change_count', 'last_50_day_50th_pct_price_change_count', 'last_50_day_70th_pct_price_change_count', 'last_50_day_40th_to_50th_pct_volume_change_count', 'last_50_day_50th_to_60th_pct_volume_change_count', 'last_50_day_50th_pct_volume_change_count', 'last_50_day_80th_pct_volume_change_count', 'last_50_day_40th_to_50th_pct_spread_count', 'last_50_day_50th_to_60th_pct_spread_count', 'last_50_day_60th_to_70th_pct_spread_count', 'last_50_day_70th_to_80th_pct_spread_count', 'last_50_day_80th_to_90th_pct_spread_count', 'last_50_day_40th_pct_spread_count', 'last_50_day_50th_pct_spread_count', 'last_50_day_70th_pct_spread_count', 'last_50_day_90th_pct_spread_count', 'rsi_100', 'last_100_day_60th_to_70th_pct_price_change_count', 'last_100_day_80th_to_90th_pct_price_change_count', 'last_100_day_90th_to_95th_pct_price_change_count', 'last_100_day_40th_pct_price_change_count', 'last_100_day_50th_pct_price_change_count', 'last_100_day_60th_pct_price_change_count', 'last_100_day_80th_pct_price_change_count', 'last_100_day_40th_to_50th_pct_volume_change_count', 'last_100_day_50th_to_60th_pct_volume_change_count', 'last_100_day_60th_to_70th_pct_volume_change_count', 'last_100_day_70th_to_80th_pct_volume_change_count', 'last_100_day_80th_to_90th_pct_volume_change_count', 'last_100_day_40th_pct_volume_change_count', 'last_100_day_60th_pct_volume_change_count', 'last_100_day_80th_pct_volume_change_count', 'last_100_day_90th_pct_volume_change_count', 'last_100_day_40th_to_50th_pct_spread_count', 'last_100_day_50th_to_60th_pct_spread_count', 'last_100_day_60th_to_70th_pct_spread_count', 'last_100_day_70th_to_80th_pct_spread_count', 'last_100_day_90th_to_95th_pct_spread_count', 'last_100_day_40th_pct_spread_count', 'last_100_day_50th_pct_spread_count', 'last_100_day_60th_pct_spread_count', 'last_100_day_80th_pct_spread_count', 'last_100_day_90th_pct_spread_count']\n",
    "    cols_for_hopsworks = ['date', 'open', 'high', 'low', 'close', 'volume', 'volume_eth', 'market_cap']\n",
    "    df = df[cols_for_hopsworks + ['tmw_1_0_percent_increase_binary', 'tmw_percent_increase_to_avg_high_low'] + predictors_96]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>volume_eth</th>\n",
       "      <th>market_cap</th>\n",
       "      <th>tmw_1_0_percent_increase_binary</th>\n",
       "      <th>tmw_percent_increase_to_avg_high_low</th>\n",
       "      <th>...</th>\n",
       "      <th>last_100_day_40th_to_50th_pct_spread_count</th>\n",
       "      <th>last_100_day_50th_to_60th_pct_spread_count</th>\n",
       "      <th>last_100_day_60th_to_70th_pct_spread_count</th>\n",
       "      <th>last_100_day_70th_to_80th_pct_spread_count</th>\n",
       "      <th>last_100_day_90th_to_95th_pct_spread_count</th>\n",
       "      <th>last_100_day_40th_pct_spread_count</th>\n",
       "      <th>last_100_day_50th_pct_spread_count</th>\n",
       "      <th>last_100_day_60th_pct_spread_count</th>\n",
       "      <th>last_100_day_80th_pct_spread_count</th>\n",
       "      <th>last_100_day_90th_pct_spread_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3083</th>\n",
       "      <td>2024-05-13</td>\n",
       "      <td>2916.796334</td>\n",
       "      <td>2984.551884</td>\n",
       "      <td>2868.448720</td>\n",
       "      <td>2948.303455</td>\n",
       "      <td>1.393028e+10</td>\n",
       "      <td>4.735937e+06</td>\n",
       "      <td>3.541556e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.010599</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3084</th>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>2945.254691</td>\n",
       "      <td>2952.313133</td>\n",
       "      <td>2867.874351</td>\n",
       "      <td>2881.795992</td>\n",
       "      <td>1.125873e+10</td>\n",
       "      <td>3.873530e+06</td>\n",
       "      <td>3.463060e+11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date         open         high          low        close  \\\n",
       "3083 2024-05-13  2916.796334  2984.551884  2868.448720  2948.303455   \n",
       "3084 2024-05-14  2945.254691  2952.313133  2867.874351  2881.795992   \n",
       "\n",
       "            volume    volume_eth    market_cap  \\\n",
       "3083  1.393028e+10  4.735937e+06  3.541556e+11   \n",
       "3084  1.125873e+10  3.873530e+06  3.463060e+11   \n",
       "\n",
       "      tmw_1_0_percent_increase_binary  tmw_percent_increase_to_avg_high_low  \\\n",
       "3083                              0.0                             -0.010599   \n",
       "3084                              NaN                                   NaN   \n",
       "\n",
       "      ...  last_100_day_40th_to_50th_pct_spread_count  \\\n",
       "3083  ...                                        14.0   \n",
       "3084  ...                                        14.0   \n",
       "\n",
       "      last_100_day_50th_to_60th_pct_spread_count  \\\n",
       "3083                                         9.0   \n",
       "3084                                         9.0   \n",
       "\n",
       "      last_100_day_60th_to_70th_pct_spread_count  \\\n",
       "3083                                        10.0   \n",
       "3084                                        10.0   \n",
       "\n",
       "      last_100_day_70th_to_80th_pct_spread_count  \\\n",
       "3083                                         8.0   \n",
       "3084                                         8.0   \n",
       "\n",
       "      last_100_day_90th_to_95th_pct_spread_count  \\\n",
       "3083                                         3.0   \n",
       "3084                                         3.0   \n",
       "\n",
       "      last_100_day_40th_pct_spread_count  last_100_day_50th_pct_spread_count  \\\n",
       "3083                                48.0                                34.0   \n",
       "3084                                48.0                                34.0   \n",
       "\n",
       "      last_100_day_60th_pct_spread_count  last_100_day_80th_pct_spread_count  \\\n",
       "3083                                25.0                                 7.0   \n",
       "3084                                25.0                                 7.0   \n",
       "\n",
       "      last_100_day_90th_pct_spread_count  \n",
       "3083                                 3.0  \n",
       "3084                                 3.0  \n",
       "\n",
       "[2 rows x 106 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eth_responses = add_response_vars(eth_combined)\n",
    "eth_predictors_and_responses = add_predictor_vars(eth_responses)\n",
    "eth_new_days_transformed = eth_predictors_and_responses.iloc[-2:]\n",
    "eth_new_days_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>volume_eth</th>\n",
       "      <th>market_cap</th>\n",
       "      <th>tmw_1_0_percent_increase_binary</th>\n",
       "      <th>tmw_percent_increase_to_avg_high_low</th>\n",
       "      <th>...</th>\n",
       "      <th>last_100_day_40th_to_50th_pct_spread_count</th>\n",
       "      <th>last_100_day_50th_to_60th_pct_spread_count</th>\n",
       "      <th>last_100_day_60th_to_70th_pct_spread_count</th>\n",
       "      <th>last_100_day_70th_to_80th_pct_spread_count</th>\n",
       "      <th>last_100_day_90th_to_95th_pct_spread_count</th>\n",
       "      <th>last_100_day_40th_pct_spread_count</th>\n",
       "      <th>last_100_day_50th_pct_spread_count</th>\n",
       "      <th>last_100_day_60th_pct_spread_count</th>\n",
       "      <th>last_100_day_80th_pct_spread_count</th>\n",
       "      <th>last_100_day_90th_pct_spread_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-11-15</td>\n",
       "      <td>0.891200</td>\n",
       "      <td>0.921500</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.906400</td>\n",
       "      <td>4.118000e+05</td>\n",
       "      <td>4.583650e+05</td>\n",
       "      <td>6.720000e+07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.033594</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-11-16</td>\n",
       "      <td>0.906200</td>\n",
       "      <td>0.944700</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.929000</td>\n",
       "      <td>6.209000e+05</td>\n",
       "      <td>6.764420e+05</td>\n",
       "      <td>6.860000e+07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.097955</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-11-17</td>\n",
       "      <td>0.924900</td>\n",
       "      <td>1.030000</td>\n",
       "      <td>0.905800</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>1.100000e+06</td>\n",
       "      <td>1.183690e+06</td>\n",
       "      <td>7.220000e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.009901</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-11-18</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>0.940500</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>6.811000e+05</td>\n",
       "      <td>6.919940e+05</td>\n",
       "      <td>7.360000e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.007323</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-11-19</td>\n",
       "      <td>0.988700</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.955500</td>\n",
       "      <td>4.435000e+05</td>\n",
       "      <td>4.558660e+05</td>\n",
       "      <td>7.280000e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.015908</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3080</th>\n",
       "      <td>2024-05-10</td>\n",
       "      <td>3035.000000</td>\n",
       "      <td>3050.000000</td>\n",
       "      <td>2889.000000</td>\n",
       "      <td>2910.000000</td>\n",
       "      <td>1.060000e+10</td>\n",
       "      <td>3.557434e+06</td>\n",
       "      <td>3.650000e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004467</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3081</th>\n",
       "      <td>2024-05-11</td>\n",
       "      <td>2909.000000</td>\n",
       "      <td>2935.000000</td>\n",
       "      <td>2894.000000</td>\n",
       "      <td>2911.000000</td>\n",
       "      <td>5.900000e+09</td>\n",
       "      <td>2.033385e+06</td>\n",
       "      <td>3.569000e+11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010306</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3082</th>\n",
       "      <td>2024-05-12</td>\n",
       "      <td>2912.000000</td>\n",
       "      <td>2951.000000</td>\n",
       "      <td>2906.000000</td>\n",
       "      <td>2931.000000</td>\n",
       "      <td>5.200000e+09</td>\n",
       "      <td>1.779026e+06</td>\n",
       "      <td>3.581000e+11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010926</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083</th>\n",
       "      <td>2024-05-13</td>\n",
       "      <td>2916.796334</td>\n",
       "      <td>2984.551884</td>\n",
       "      <td>2868.448720</td>\n",
       "      <td>2948.303455</td>\n",
       "      <td>1.393028e+10</td>\n",
       "      <td>4.735937e+06</td>\n",
       "      <td>3.541556e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.009737</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3084</th>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>2945.254691</td>\n",
       "      <td>2952.313133</td>\n",
       "      <td>2867.874351</td>\n",
       "      <td>2881.795992</td>\n",
       "      <td>1.125873e+10</td>\n",
       "      <td>3.873530e+06</td>\n",
       "      <td>3.463060e+11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3085 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date         open         high          low        close  \\\n",
       "0    2015-11-15     0.891200     0.921500     0.875000     0.906400   \n",
       "1    2015-11-16     0.906200     0.944700     0.892000     0.929000   \n",
       "2    2015-11-17     0.924900     1.030000     0.905800     1.010000   \n",
       "3    2015-11-18     0.990000     1.010000     0.940500     0.990000   \n",
       "4    2015-11-19     0.988700     1.010000     0.937500     0.955500   \n",
       "...         ...          ...          ...          ...          ...   \n",
       "3080 2024-05-10  3035.000000  3050.000000  2889.000000  2910.000000   \n",
       "3081 2024-05-11  2909.000000  2935.000000  2894.000000  2911.000000   \n",
       "3082 2024-05-12  2912.000000  2951.000000  2906.000000  2931.000000   \n",
       "3083 2024-05-13  2916.796334  2984.551884  2868.448720  2948.303455   \n",
       "3084 2024-05-14  2945.254691  2952.313133  2867.874351  2881.795992   \n",
       "\n",
       "            volume    volume_eth    market_cap  \\\n",
       "0     4.118000e+05  4.583650e+05  6.720000e+07   \n",
       "1     6.209000e+05  6.764420e+05  6.860000e+07   \n",
       "2     1.100000e+06  1.183690e+06  7.220000e+07   \n",
       "3     6.811000e+05  6.919940e+05  7.360000e+07   \n",
       "4     4.435000e+05  4.558660e+05  7.280000e+07   \n",
       "...            ...           ...           ...   \n",
       "3080  1.060000e+10  3.557434e+06  3.650000e+11   \n",
       "3081  5.900000e+09  2.033385e+06  3.569000e+11   \n",
       "3082  5.200000e+09  1.779026e+06  3.581000e+11   \n",
       "3083  1.393028e+10  4.735937e+06  3.541556e+11   \n",
       "3084  1.125873e+10  3.873530e+06  3.463060e+11   \n",
       "\n",
       "      tmw_1_0_percent_increase_binary  tmw_percent_increase_to_avg_high_low  \\\n",
       "0                                 1.0                              0.033594   \n",
       "1                                 1.0                              0.097955   \n",
       "2                                 0.0                             -0.009901   \n",
       "3                                 0.0                             -0.007323   \n",
       "4                                 0.0                             -0.015908   \n",
       "...                               ...                                   ...   \n",
       "3080                              0.0                              0.004467   \n",
       "3081                              1.0                              0.010306   \n",
       "3082                              1.0                              0.010926   \n",
       "3083                              0.0                             -0.009737   \n",
       "3084                              NaN                                   NaN   \n",
       "\n",
       "      ...  last_100_day_40th_to_50th_pct_spread_count  \\\n",
       "0     ...                                         8.0   \n",
       "1     ...                                         8.0   \n",
       "2     ...                                         8.0   \n",
       "3     ...                                         8.0   \n",
       "4     ...                                         8.0   \n",
       "...   ...                                         ...   \n",
       "3080  ...                                        14.0   \n",
       "3081  ...                                        14.0   \n",
       "3082  ...                                        14.0   \n",
       "3083  ...                                        14.0   \n",
       "3084  ...                                        14.0   \n",
       "\n",
       "      last_100_day_50th_to_60th_pct_spread_count  \\\n",
       "0                                            7.0   \n",
       "1                                            8.0   \n",
       "2                                            8.0   \n",
       "3                                            8.0   \n",
       "4                                            8.0   \n",
       "...                                          ...   \n",
       "3080                                         9.0   \n",
       "3081                                         9.0   \n",
       "3082                                         9.0   \n",
       "3083                                         9.0   \n",
       "3084                                         9.0   \n",
       "\n",
       "      last_100_day_60th_to_70th_pct_spread_count  \\\n",
       "0                                            6.0   \n",
       "1                                            6.0   \n",
       "2                                            6.0   \n",
       "3                                            7.0   \n",
       "4                                            8.0   \n",
       "...                                          ...   \n",
       "3080                                        10.0   \n",
       "3081                                        10.0   \n",
       "3082                                        10.0   \n",
       "3083                                        10.0   \n",
       "3084                                        10.0   \n",
       "\n",
       "      last_100_day_70th_to_80th_pct_spread_count  \\\n",
       "0                                           18.0   \n",
       "1                                           18.0   \n",
       "2                                           18.0   \n",
       "3                                           18.0   \n",
       "4                                           18.0   \n",
       "...                                          ...   \n",
       "3080                                         8.0   \n",
       "3081                                         8.0   \n",
       "3082                                         8.0   \n",
       "3083                                         8.0   \n",
       "3084                                         8.0   \n",
       "\n",
       "      last_100_day_90th_to_95th_pct_spread_count  \\\n",
       "0                                           18.0   \n",
       "1                                           18.0   \n",
       "2                                           18.0   \n",
       "3                                           17.0   \n",
       "4                                           17.0   \n",
       "...                                          ...   \n",
       "3080                                         3.0   \n",
       "3081                                         3.0   \n",
       "3082                                         3.0   \n",
       "3083                                         3.0   \n",
       "3084                                         3.0   \n",
       "\n",
       "      last_100_day_40th_pct_spread_count  last_100_day_50th_pct_spread_count  \\\n",
       "0                                   94.0                                86.0   \n",
       "1                                   94.0                                86.0   \n",
       "2                                   94.0                                86.0   \n",
       "3                                   94.0                                86.0   \n",
       "4                                   94.0                                86.0   \n",
       "...                                  ...                                 ...   \n",
       "3080                                48.0                                34.0   \n",
       "3081                                48.0                                34.0   \n",
       "3082                                48.0                                34.0   \n",
       "3083                                48.0                                34.0   \n",
       "3084                                48.0                                34.0   \n",
       "\n",
       "      last_100_day_60th_pct_spread_count  last_100_day_80th_pct_spread_count  \\\n",
       "0                                   79.0                                55.0   \n",
       "1                                   78.0                                54.0   \n",
       "2                                   78.0                                54.0   \n",
       "3                                   78.0                                53.0   \n",
       "4                                   78.0                                52.0   \n",
       "...                                  ...                                 ...   \n",
       "3080                                25.0                                 7.0   \n",
       "3081                                25.0                                 7.0   \n",
       "3082                                25.0                                 7.0   \n",
       "3083                                25.0                                 7.0   \n",
       "3084                                25.0                                 7.0   \n",
       "\n",
       "      last_100_day_90th_pct_spread_count  \n",
       "0                                   46.0  \n",
       "1                                   45.0  \n",
       "2                                   44.0  \n",
       "3                                   43.0  \n",
       "4                                   42.0  \n",
       "...                                  ...  \n",
       "3080                                 3.0  \n",
       "3081                                 3.0  \n",
       "3082                                 3.0  \n",
       "3083                                 3.0  \n",
       "3084                                 3.0  \n",
       "\n",
       "[3085 rows x 106 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace rows in eth_old with rows from eth_new_days_transformed based on the 'date' column\n",
    "eth_old2 = eth_old.copy()\n",
    "eth_old2.loc[eth_old2['date'].isin(eth_new_days_transformed['date'])] = eth_new_days_transformed.loc[eth_new_days_transformed['date'].isin(eth_old2['date'])]\n",
    "\n",
    "# Find rows in eth_new_days_transformed that are not in eth_old\n",
    "unique_dates_mask = ~eth_new_days_transformed['date'].isin(eth_old2['date'])\n",
    "eth_new_unique = eth_new_days_transformed[unique_dates_mask]\n",
    "\n",
    "# Concatenate the updated eth_old with the unique new rows\n",
    "eth_transformed = pd.concat([eth_old, eth_new_unique], ignore_index=True)\n",
    "\n",
    "eth_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data back into Hopsworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection closed.\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "UserWarning: The installed hopsworks client version 3.4.4 may not be compatible with the connected Hopsworks backend version 3.7.1. \n",
      "To ensure compatibility please install the latest bug fix release matching the minor version of your backend (3.7) by running 'pip install hopsworks==3.7.*'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/711829\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 3085/3085 | Elapsed Time: 00:03 | Remaining Time: 00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: eth_ohlc_transformed_2_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai/p/711829/jobs/named/eth_ohlc_transformed_2_offline_fg_materialization/executions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eth_transformed['tmw_1_0_percent_increase_binary'] = eth_transformed['tmw_1_0_percent_increase_binary'].astype('Int64')\n",
    "\n",
    "upload_data(eth_transformed, 'eth_ohlc_transformed', 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_crypto_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
