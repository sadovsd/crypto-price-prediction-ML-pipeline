{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /Users/davydsadovskyy/GitBackedShit/crypto-prediction/notebooks\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from src.hopsworks_connections import pull_data, upload_data\n",
    "import pandas_ta as ta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print the current working directory\n",
    "current_working_directory = os.getcwd()\n",
    "print(\"Current Working Directory:\", current_working_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get raw ethereum ohlc data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_raw_data(filename):\n",
    "    eth_ohlc_raw = pd.read_csv(f'../data/raw/ohlc/{filename}', parse_dates=True)\n",
    "    def convert_value(value):\n",
    "        \"\"\"\n",
    "        Converts a string value to a float. Removes $ signs, and converts\n",
    "        billion (bn), million (m), and thousand (K) values to their numeric equivalents.\n",
    "        \"\"\"\n",
    "        value = value.replace('$', '')  # Remove $ sign to simplify processing\n",
    "        if value[-1].lower() == 'm':\n",
    "            return float(value[:-1]) * 1_000_000\n",
    "        elif value[-1].lower() == 'b':\n",
    "            return float(value[:-1]) * 1_000_000_000\n",
    "        elif value[-1].lower() == 'k':\n",
    "            return float(value[:-1]) * 1_000\n",
    "        elif value[-2:].lower() == 'bn':  # Handle 'bn' for billions\n",
    "            return float(value[:-2]) * 1_000_000_000\n",
    "        else:\n",
    "            return float(value)\n",
    "\n",
    "    eth_ohlc_raw.columns = [col.lower() for col in eth_ohlc_raw.columns]\n",
    "    eth_ohlc_raw.rename(columns={'volume(eth)': 'volume_eth', 'market cap': 'market_cap'}, inplace=True)\n",
    "\n",
    "    for col in ['open', 'high', 'low', 'close', 'volume', 'market_cap']:\n",
    "        eth_ohlc_raw[col] = eth_ohlc_raw[col].apply(convert_value)\n",
    "    eth_ohlc_raw['date'] = pd.to_datetime(eth_ohlc_raw['date'])\n",
    "\n",
    "    return eth_ohlc_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = get_cleaned_raw_data('ethereum.csv')\n",
    "df_raw = df_raw.iloc[::-1].reset_index(drop=True) # reverse order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add response variables - these will be used to also generate some predictors later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_response_variables(df):\n",
    "    \n",
    "    df_tranformed = df.copy()\n",
    "    df_tranformed = df_tranformed.iloc[::-1].reset_index(drop=True) # reverse order\n",
    "\n",
    "    # Create binary response variable for percent returns tomorrow\n",
    "    df_tranformed[\"tmw_avg_high_close\"] = (df_tranformed[\"high\"].shift(-1) + df_tranformed[\"close\"].shift(-1)) / 2\n",
    "    df_tranformed['tmw_percent_increase'] = ((df_tranformed['tmw_avg_high_close'] - df_tranformed['close']) / df_tranformed['tmw_avg_high_close'])\n",
    "    intervals = [1 + i * 0.25 for i in range(int((3 - 1) / 0.25) + 1)] # 1-3, by .25\n",
    "    for i in intervals:\n",
    "        threshold = i / 100\n",
    "        column_name = f\"tmw_{str(i).replace('.', '_')}_percent_increase_binary\"\n",
    "        df_tranformed[column_name] = (df_tranformed['tmw_percent_increase'] >= threshold).astype(int)\n",
    "\n",
    "    return df_tranformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3162"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_response_vars = add_response_variables(df_raw)\n",
    "len(df_with_response_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding simple predictors based on momentum of the price and seasonality: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_simple_predictors(df_with_response_vars):\n",
    "    \n",
    "    df_transformed = df_with_response_vars.copy()\n",
    "\n",
    "    ###### Predictor variables about the price momentum\n",
    "\n",
    "    horizons = [2,5,10,25,50,100] \n",
    "    predictors = []\n",
    "    for horizon in horizons:\n",
    "        ### Exponential Moving Average (EMA) for past horizon\n",
    "        ema_col = f\"ema_{horizon}\"\n",
    "        df_transformed[ema_col] = df_transformed[\"close\"] / ta.ema(df_transformed[\"close\"], length=horizon)\n",
    "        predictors.append(ema_col)\n",
    "\n",
    "        ### Relative Strength Index (RSI) for past horizon\n",
    "        rsi_col = f\"rsi_{horizon}\"\n",
    "        df_transformed[rsi_col] = ta.rsi(df_transformed[\"close\"], length=horizon)\n",
    "        predictors.append(rsi_col)\n",
    "\n",
    "        ### Simple Moving Average (SMA) for past horizon\n",
    "        sma_col = f\"sma_{horizon}\"\n",
    "        df_transformed[sma_col] = df_transformed[\"close\"] / ta.sma(df_transformed[\"close\"], length=horizon)\n",
    "        predictors.append(sma_col)\n",
    "\n",
    "        ### Sum of the number of days that had a certain percent increase for the past horizon\n",
    "        columns = [\n",
    "            # 'tmw_positive_percent_increase_binary',\n",
    "            'tmw_1_0_percent_increase_binary',\n",
    "            'tmw_1_25_percent_increase_binary',\n",
    "            'tmw_1_5_percent_increase_binary',\n",
    "            'tmw_1_75_percent_increase_binary',\n",
    "            'tmw_2_0_percent_increase_binary',\n",
    "            'tmw_2_25_percent_increase_binary',\n",
    "            'tmw_2_5_percent_increase_binary',\n",
    "            'tmw_2_75_percent_increase_binary',\n",
    "            'tmw_3_0_percent_increase_binary'\n",
    "        ]\n",
    "        for col in columns:\n",
    "            percent = col.split('tmw_')[1].split('_percent')[0]\n",
    "            trend_col = f\"last_{horizon}_day_{percent}_percent_increase_count\"\n",
    "            if col in df_transformed.columns:\n",
    "                df_transformed[trend_col] = df_transformed[col].shift(1).rolling(horizon).sum()\n",
    "            else:\n",
    "                print('PREDICTORS NOT ADDED - You must run add_response_vars() first so that predictors can be created based on that info')\n",
    "                break    \n",
    "\n",
    "    ###### Predictor variables about seasonality/cycles\n",
    "\n",
    "    month_names = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december']\n",
    "    for i, month in enumerate(month_names):\n",
    "        df_transformed[month] = (df_transformed['date'].dt.month == i).astype(int)\n",
    "\n",
    "    days = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "    for i, day in enumerate(days):\n",
    "        df_transformed[day] = (df_transformed['date'].dt.dayofweek == i).astype(int)\n",
    "\n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3162"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_response_and_simple_preds = add_simple_predictors(df_with_response_vars)\n",
    "len(df_with_response_and_simple_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3061"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop NA values\n",
    "df_with_response_and_simple_preds = df_with_response_and_simple_preds.dropna()\n",
    "len(df_with_response_and_simple_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding predictors with insight from the paper \"101 Formulaic Alphas, by Zura Kakushadze, 2015\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Add columns that will be inputs to the 101 formulaic alphas the author defines\n",
    "\n",
    "df_with_advanced_alphas_predictors = df_raw.copy()\n",
    "\n",
    "# adv{d} = average daily dollar volume for the past d days\n",
    "periods = [2, 5, 10, 25, 50, 100]\n",
    "for period in periods:\n",
    "\n",
    "    # Calculate adv{d} = average daily dollar volume for the past d days\n",
    "    column_name = f'adv_{period}'\n",
    "    df_with_advanced_alphas_predictors[column_name] = (df_with_advanced_alphas_predictors['volume'] \n",
    "                                                         * df_with_advanced_alphas_predictors['close']).rolling(window=period).mean()\n",
    "    \n",
    "    # Calculate Volume-Weighted Average Price (VWAP)    \n",
    "    column_name = f'vwap_{period}'\n",
    "    df_with_advanced_alphas_predictors[column_name] = (\n",
    "        df_with_advanced_alphas_predictors['close'] * df_with_advanced_alphas_predictors['volume']\n",
    "    ).rolling(window=period).sum() / df_with_advanced_alphas_predictors['volume'].rolling(window=period).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Functions necessary for transforming input into alphas (predictors)\n",
    "\n",
    "def rank(x, ascending=False):\n",
    "    return df_with_advanced_alphas_predictors[x].rank(ascending=ascending)\n",
    "\n",
    "def delta(x, days):\n",
    "    return df_with_advanced_alphas_predictors[x] - df_with_advanced_alphas_predictors[x].shift(days)\n",
    "\n",
    "def correlation(x, y, d):\n",
    "    return df_with_advanced_alphas_predictors[x].rolling(window=d).corr(df_with_advanced_alphas_predictors[y])\n",
    "\n",
    "def log(x):\n",
    "    return np.log(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RuntimeWarning: invalid value encountered in log\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index([ 12.00944930402536, 13.421282081782097, 13.184774637525726,\\n         12.9123828126164,  14.22097566607244, 14.603967918328545,\\n       15.226497531674536, 15.341566861459324,  14.77102200299171,\\n       15.096444403426338,\\n       ...\\n       23.693680302516114, 24.004177052734065,  23.85876005287556,\\n       23.662427759012008, 23.562344300455024, 23.464105860871612,\\n       23.325955522390796, 23.579736043166893, 22.909317113684505,\\n        22.82739999121662],\\n      dtype='float64', length=3162)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[163], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Alpha#2: (-1 * correlation(rank(delta(log(volume), 2)), rank(((close - open) / open)), 6))\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_with_advanced_alphas_predictors[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha_2\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m correlation(rank(\u001b[43mdelta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvolume\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m), \n\u001b[1;32m      3\u001b[0m                                                       rank(((df_with_advanced_alphas_predictors[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m \n\u001b[1;32m      4\u001b[0m                                                              df_with_advanced_alphas_predictors[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopen\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m/\u001b[39m \n\u001b[1;32m      5\u001b[0m                                                              df_with_advanced_alphas_predictors[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopen\u001b[39m\u001b[38;5;124m'\u001b[39m])), \u001b[38;5;241m6\u001b[39m)\n\u001b[1;32m      7\u001b[0m df_with_advanced_alphas_predictors[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha_2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[162], line 7\u001b[0m, in \u001b[0;36mdelta\u001b[0;34m(x, days)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdelta\u001b[39m(x, days):\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdf_with_advanced_alphas_predictors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m-\u001b[39m df_with_advanced_alphas_predictors[x]\u001b[38;5;241m.\u001b[39mshift(days)\n",
      "File \u001b[0;32m~/GitBackedShit/crypto-prediction/venv_crypto_prediction/lib/python3.10/site-packages/pandas/core/frame.py:3899\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3898\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3899\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3901\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/GitBackedShit/crypto-prediction/venv_crypto_prediction/lib/python3.10/site-packages/pandas/core/indexes/base.py:6115\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6113\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6115\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6117\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6119\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/GitBackedShit/crypto-prediction/venv_crypto_prediction/lib/python3.10/site-packages/pandas/core/indexes/base.py:6176\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   6175\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 6176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6178\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6179\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index([ 12.00944930402536, 13.421282081782097, 13.184774637525726,\\n         12.9123828126164,  14.22097566607244, 14.603967918328545,\\n       15.226497531674536, 15.341566861459324,  14.77102200299171,\\n       15.096444403426338,\\n       ...\\n       23.693680302516114, 24.004177052734065,  23.85876005287556,\\n       23.662427759012008, 23.562344300455024, 23.464105860871612,\\n       23.325955522390796, 23.579736043166893, 22.909317113684505,\\n        22.82739999121662],\\n      dtype='float64', length=3162)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# Alpha#2: (-1 * correlation(rank(delta(log(volume), 2)), rank(((close - open) / open)), 6))\n",
    "df_with_advanced_alphas_predictors[\"alpha_2\"] = -1 * correlation(rank(delta(log('volume'), 2)), \n",
    "                                                      rank(((df_with_advanced_alphas_predictors['close'] - \n",
    "                                                             df_with_advanced_alphas_predictors['open']) / \n",
    "                                                             df_with_advanced_alphas_predictors['open'])), 6)\n",
    "\n",
    "df_with_advanced_alphas_predictors[\"alpha_2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get set of predictors that have low correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors = []\n",
    "periods = [2, 5, 10, 25, 50, 100]\n",
    "percent_increase_counts = ['1_0', '1_25', '1_5', '1_75', '2_0', '2_25', '2_5', '2_75', '3_0']\n",
    "for period in periods:\n",
    "    for indicator in ['ema', 'rsi', 'sma']:\n",
    "        predictors.append(f'{indicator}_{period}')\n",
    "    for percent in percent_increase_counts:\n",
    "        predictors.append(f'last_{period}_day_{percent}_percent_increase_count')\n",
    "months = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december']\n",
    "predictors.extend(months)\n",
    "weekdays = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "predictors.extend(weekdays)\n",
    "len(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a list of noncorrelated predictors\n",
    "def drop_predictors_with_high_correlation(data, predictors_list, correlation_threshold):\n",
    "\n",
    "    corr_matrix = data[predictors_list].corr()\n",
    "\n",
    "    # Threshold for high correlation\n",
    "    high_corr_threshold = correlation_threshold\n",
    "\n",
    "    high_corr_pairs = []\n",
    "    for col in corr_matrix.columns:\n",
    "        for row in corr_matrix.index:\n",
    "            if col != row and corr_matrix.at[row, col] > high_corr_threshold:\n",
    "                # Sort the pair to avoid duplicates like (A, B) and (B, A)\n",
    "                pair = tuple(sorted([row, col]))\n",
    "                if pair not in high_corr_pairs:\n",
    "                    high_corr_pairs.append(pair)\n",
    "\n",
    "    # Identify and remove correlation pairs if the second element occurs as a first element in the list of all pairs\n",
    "    first_elements = [pair[0] for pair in high_corr_pairs]\n",
    "    filtered_pairs = [pair for pair in high_corr_pairs if pair[1] not in first_elements]\n",
    "\n",
    "    # Find the unique first elements of all the pairs\n",
    "    predictors_high_cor = list(set(pair[0] for pair in filtered_pairs))\n",
    "\n",
    "    print(\"original number of predictors in model: \", len(predictors))\n",
    "    print(\"number of unique highly correlated predictors to be dropped from model: \", len(predictors_high_cor))\n",
    "\n",
    "    # Removing elements from predictors that are in predictors_high_cor\n",
    "    predictors_low_cor = [x for x in predictors if x not in predictors_high_cor]\n",
    "    len(predictors_low_cor)\n",
    "\n",
    "    print(\"number of predictors remaining in model: \", len(predictors_low_cor))\n",
    "\n",
    "    return predictors_low_cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original number of predictors in model:  91\n",
      "number of unique highly correlated predictors to be dropped from model:  32\n",
      "number of predictors remaining in model:  59\n",
      "\n",
      " ['rsi_2', 'sma_2', 'last_2_day_1_0_percent_increase_count', 'last_2_day_1_25_percent_increase_count', 'last_2_day_1_5_percent_increase_count', 'last_2_day_1_75_percent_increase_count', 'last_2_day_2_0_percent_increase_count', 'last_2_day_3_0_percent_increase_count', 'rsi_5', 'sma_5', 'last_5_day_1_0_percent_increase_count', 'last_5_day_1_25_percent_increase_count', 'last_5_day_1_5_percent_increase_count', 'last_5_day_1_75_percent_increase_count', 'last_5_day_2_0_percent_increase_count', 'last_5_day_3_0_percent_increase_count', 'rsi_10', 'last_10_day_1_0_percent_increase_count', 'last_10_day_1_25_percent_increase_count', 'last_10_day_1_5_percent_increase_count', 'last_10_day_1_75_percent_increase_count', 'last_10_day_3_0_percent_increase_count', 'last_25_day_1_0_percent_increase_count', 'last_25_day_1_25_percent_increase_count', 'last_25_day_1_5_percent_increase_count', 'last_25_day_3_0_percent_increase_count', 'sma_50', 'last_50_day_1_0_percent_increase_count', 'last_50_day_1_25_percent_increase_count', 'last_50_day_1_5_percent_increase_count', 'last_50_day_3_0_percent_increase_count', 'rsi_100', 'last_100_day_1_0_percent_increase_count', 'last_100_day_1_25_percent_increase_count', 'last_100_day_1_5_percent_increase_count', 'last_100_day_1_75_percent_increase_count', 'last_100_day_2_0_percent_increase_count', 'last_100_day_2_25_percent_increase_count', 'last_100_day_2_5_percent_increase_count', 'last_100_day_2_75_percent_increase_count', 'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n"
     ]
    }
   ],
   "source": [
    "predictors_low_cor = drop_predictors_with_high_correlation(df_with_response_and_simple_preds, predictors, 0.85)\n",
    "print('\\n', predictors_low_cor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision_score(data, model, predictors, response_var, threshold, start=1000, step=100):\n",
    "    \n",
    "    def predict(train, test, predictors, model):\n",
    "        model.fit(train[predictors], train[response_var])\n",
    "        probability = model.predict_proba(test[predictors])[:,1]\n",
    "        proba_series = pd.Series(probability, index=test.index, name=\"probability\")\n",
    "        combined = pd.concat([test[response_var], test['tmw_percent_increase'], proba_series], axis=1)\n",
    "        return combined\n",
    "\n",
    "    all_predictions = []\n",
    "    for i in range(start, data.shape[0], step):\n",
    "        train = data.iloc[0:i].copy()\n",
    "        test = data.iloc[i:(i+100)].copy()\n",
    "        predictions = predict(train, test, predictors, model)\n",
    "        all_predictions.append(predictions)\n",
    "\n",
    "    data_with_predictions = pd.concat(all_predictions)\n",
    "\n",
    "    col_name = f'pred_{threshold}'\n",
    "    data_with_predictions[col_name] = data_with_predictions['probability'] >= threshold\n",
    "\n",
    "    ppv = precision_score(data_with_predictions[response_var], data_with_predictions[col_name])\n",
    "    \n",
    "    return ppv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6113074204946997"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=250, min_samples_split=200, random_state=1)\n",
    "response_var = 'tmw_1_0_percent_increase_binary'\n",
    "threshold = 0.55\n",
    "\n",
    "ppv = get_precision_score(df_with_response_and_simple_preds, model, predictors_low_cor, response_var, threshold)\n",
    "ppv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_crypto_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
